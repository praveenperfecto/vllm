{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4e00e6-b9e9-4d5c-8281-5f9587459137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vllm in /usr/local/lib/python3.10/dist-packages (0.4.0.post1)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from vllm) (1.11.1.1)\n",
      "Requirement already satisfied: pynvml==11.5.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (11.5.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm) (5.9.8)\n",
      "Requirement already satisfied: ray>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.11.0)\n",
      "Requirement already satisfied: torch==2.1.2 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.1.2)\n",
      "Requirement already satisfied: transformers>=4.39.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.40.0)\n",
      "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.20.0)\n",
      "Requirement already satisfied: cmake>=3.21 in /usr/local/lib/python3.10/dist-packages (from vllm) (3.29.2)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from vllm) (0.110.2)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.7.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vllm) (1.26.4)\n",
      "Requirement already satisfied: xformers==0.0.23.post1 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.0.23.post1)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vllm) (2.31.0)\n",
      "Requirement already satisfied: outlines==0.0.34 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.0.34)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm) (0.2.0)\n",
      "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.10/dist-packages (from vllm) (0.29.0)\n",
      "Requirement already satisfied: tiktoken==0.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.6.0)\n",
      "Requirement already satisfied: triton>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.1.0)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (0.34.0)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (1.6.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (4.21.1)\n",
      "Requirement already satisfied: interegular in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (0.3.3)\n",
      "Requirement already satisfied: lark in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (1.1.9)\n",
      "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (5.6.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (1.13.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (3.0.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (1.4.0)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (0.59.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (3.1.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.6.0->vllm) (2024.4.16)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (12.1.105)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (12.1.105)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (1.12)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (11.4.5.107)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (3.13.4)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (11.0.2.54)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->vllm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->vllm) (12.4.127)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->vllm) (2.18.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->vllm) (0.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (24.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (8.1.7)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (5.26.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.0.8)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.3.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (6.0.1)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vllm) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vllm) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vllm) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vllm) (3.3.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.1->vllm) (0.22.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.1->vllm) (4.66.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.1->vllm) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.1->vllm) (0.19.1)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->vllm) (0.37.2)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.14.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.21.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.6.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (12.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.38.0,>=0.37.2->fastapi->vllm) (4.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.0.34->vllm) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.34->vllm) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.34->vllm) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.34->vllm) (0.18.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines==0.0.34->vllm) (0.42.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->vllm) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi->vllm) (1.2.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi->vllm) (1.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55a3a9-3a16-4354-ba37-1f1989376dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74abb9e2-d8ea-4381-93da-c83506179290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-22 09:31:32,104\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 09:31:32 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-125m', tokenizer='facebook/opt-125m', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 04-22 09:31:33 selector.py:51] Cannot use FlashAttention because the package is not found. Please install it for better performance.\n",
      "INFO 04-22 09:31:33 selector.py:25] Using XFormers backend.\n",
      "INFO 04-22 09:31:36 weight_utils.py:177] Using model weights format ['*.bin']\n",
      "INFO 04-22 09:31:36 model_runner.py:104] Loading model weights took 0.2389 GB\n",
      "INFO 04-22 09:31:37 gpu_executor.py:94] # GPU blocks: 34411, # CPU blocks: 7281\n",
      "INFO 04-22 09:31:40 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-22 09:31:40 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-22 09:31:43 model_runner.py:867] Graph capturing finished in 3 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 42.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: ' Joel, my dad is my friend and we are in a relationship. I am'\n",
      "Prompt: 'The president of the United States is', Generated text: ' speaking out against the release of some State Department documents which show the Russians are trying'\n",
      "Prompt: 'The capital of France is', Generated text: ' the most populous city in the world, with an annual population of nearly 3 million'\n",
      "Prompt: 'The future of AI is', Generated text: ' at stake\\nThe world is going to change in the next 20 years, and'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Sample prompts.\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "# Create an LLM.\n",
    "llm = LLM(model=\"facebook/opt-125m\")\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abf0f0-8b6f-44fe-94fa-d56ba015b3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187ea040-61d3-4f7f-bf29-733d40a43d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e3ea4-b9cf-4065-8c6e-0a89e0091bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc5ac3e-7a41-4d3c-98ad-839e30fe8d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8f79e2-2d22-449d-bdc0-a2e7b976c3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d71a50-4920-4623-862f-e4238da9606c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11dfaf6a-6876-41d3-9d8f-e2d6d666d549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-22 10:14:46,401\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 10:14:48 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='microsoft/phi-2', tokenizer='microsoft/phi-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 10:14:54 selector.py:51] Cannot use FlashAttention because the package is not found. Please install it for better performance.\n",
      "INFO 04-22 10:14:54 selector.py:25] Using XFormers backend.\n",
      "INFO 04-22 10:14:58 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 04-22 10:15:00 model_runner.py:104] Loading model weights took 5.1933 GB\n",
      "INFO 04-22 10:15:01 gpu_executor.py:94] # GPU blocks: 2840, # CPU blocks: 819\n",
      "INFO 04-22 10:15:03 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-22 10:15:03 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-22 10:15:08 model_runner.py:867] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 2.0803465843200684seconds\n",
      "Throughput: 42.300643875050056tokens/second\n",
      " [1, 2, 3, 4, 5]\n",
      "A: def sum_list(numbers):\n",
      "    total = 0\n",
      "    for num in numbers:\n",
      "        total += num\n",
      "    return total\n",
      "\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "result = sum_list(numbers)\n",
      "print(result)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "\n",
    "\"\"\" VLLM transformers model loading \"\"\"\n",
    "llm = LLM(\"microsoft/phi-2\", trust_remote_code = True)\n",
    "\n",
    "prompt = \"Generate a python code that accepts a list of numbers and returns the sum.\"\n",
    "sampling_params = SamplingParams(temperature = 0.5, max_tokens = 200)\n",
    "\n",
    "start = time.time()\n",
    "response = llm.generate(prompt, sampling_params)\n",
    "end = time.time()\n",
    "\n",
    "# Latency: time required to generate response for a prompt\n",
    "latency = end-start\n",
    "print(f\"Latency: {latency}seconds\")\n",
    "\n",
    "# Throughput: tokens generated per second\n",
    "output_tokens = len(response[0].outputs[0].token_ids)\n",
    "through_put = output_tokens / latency\n",
    "print(f\"Throughput: {through_put}tokens/second\")\n",
    "\n",
    "generated_text = response[0].outputs[0].text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e24f42-fea3-4c40-b408-31e173fa1242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d1721-342f-4cd6-bb4d-2e48b45f63d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22cd4e-c50e-4b89-a6ed-276719fc01c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe00a74-084a-4c29-8c79-4d6dc133aad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf0d76-9a46-401c-aff4-01230dfadf44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab91402a-c085-49df-8a8e-d3a5cbb5ece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-25 09:58:23 pynccl.py:58] Loading nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 09:58:24,248\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-25 09:58:24 __init__.py:111] Model architecture T5ForCausalLM is already registered, and will be overwritten by the new model class T5ForVLLM.\n"
     ]
    }
   ],
   "source": [
    "from vllm.model_executor.models import ModelRegistry\n",
    "from vllm.model_executor.models.t5 import T5ForVLLM\n",
    "\n",
    "# Registering the T5 model dynamically\n",
    "ModelRegistry.register_model(\"T5ForCausalLM\", T5ForVLLM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e98b2-b638-4bb1-8e9e-5d632201632b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8cef97-dc0a-42c2-922f-f9137ffbc477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model class for architecture: T5ForCausalLM\n",
      "T5 model class loaded successfully: <class 'vllm.model_executor.models.t5.T5ForVLLM'>\n"
     ]
    }
   ],
   "source": [
    "from vllm.model_executor.models import ModelRegistry\n",
    "\n",
    "# Attempt to load the T5 model class\n",
    "t5_model_class = ModelRegistry.load_model_cls(\"T5ForCausalLM\")\n",
    "if t5_model_class:\n",
    "    print(\"T5 model class loaded successfully:\", t5_model_class)\n",
    "else:\n",
    "    print(\"Failed to load T5 model class.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba49a2a-7d74-44fd-9580-228b4a91cc2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/lib/python3.10/dist-packages/ray/thirdparty_files', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46424b9d-48c2-419d-b997-909d42ce9ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a63ae4f6-03c1-444b-bcc5-9b6e8a64d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/data/vllm\n",
      "__init__.py  decilm.py\t     gpt_neox.py   mixtral_quant.py  qwen2.py\n",
      "__pycache__  deepseek.py     internlm2.py  mpt.py\t     qwen2_moe.py\n",
      "baichuan.py  falcon.py\t     jais.py\t   olmo.py\t     stablelm.py\n",
      "bloom.py     gemma.py\t     llama.py\t   opt.py\t     starcoder2.py\n",
      "chatglm.py   gpt2.py\t     llava.py\t   orion.py\t     t5.py\n",
      "commandr.py  gpt_bigcode.py  minicpm.py    phi.py\t     xverse.py\n",
      "dbrx.py      gpt_j.py\t     mixtral.py    qwen.py\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls vllm/model_executor/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb74aa98-e8e8-4a3a-beed-1f2e2adcfbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute path for model directory: /data/data/vllm/vllm/model_executor/models/t5-small-model\n",
      "Absolute path for tokenizer directory: /data/data/vllm/vllm/model_executor/models/t5-small-tokenizer\n",
      "Model directory does not exist.\n",
      "Tokenizer directory does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Relative paths for model and tokenizer directories\n",
    "model_dir = 'vllm/model_executor/models/t5-small-model'\n",
    "tokenizer_dir = 'vllm/model_executor/models/t5-small-tokenizer'\n",
    "\n",
    "# Convert relative paths to absolute paths\n",
    "absolute_model_dir = os.path.abspath(model_dir)\n",
    "absolute_tokenizer_dir = os.path.abspath(tokenizer_dir)\n",
    "\n",
    "# Print absolute paths\n",
    "print(\"Absolute path for model directory:\", absolute_model_dir)\n",
    "print(\"Absolute path for tokenizer directory:\", absolute_tokenizer_dir)\n",
    "\n",
    "# Check and print contents of the directories if they exist\n",
    "if os.path.exists(absolute_model_dir):\n",
    "    print(\"Model files:\", os.listdir(absolute_model_dir))\n",
    "else:\n",
    "    print(\"Model directory does not exist.\")\n",
    "\n",
    "if os.path.exists(absolute_tokenizer_dir):\n",
    "    print(\"Tokenizer files:\", os.listdir(absolute_tokenizer_dir))\n",
    "else:\n",
    "    print(\"Tokenizer directory does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb4c12c1-c55b-4c60-87f8-46e53c0a85cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/data/vllm'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb749b9-f34e-4ebb-8d8f-07033187ecad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-25 10:22:03 pynccl.py:58] Loading nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:22:03,486\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: ['Comment êtes-vous, ce qui se passe, dites-moi tellement de nouveauté?']\n"
     ]
    }
   ],
   "source": [
    "from vllm.model_executor.models.t5 import T5ForVLLM\n",
    "\n",
    "# Using absolute paths for clarity\n",
    "model_dir = '/data/data/vllm/vllm/model_executor/models/t5-small-model/'\n",
    "tokenizer_dir = '/data/data/vllm/vllm/model_executor/models/t5-small-tokenizer/'\n",
    "\n",
    "# Initialize the model\n",
    "model = T5ForVLLM(model_dir, tokenizer_dir)\n",
    "\n",
    "# Test input\n",
    "# test_input = \"Translate English to French: How are you?\"\n",
    "\n",
    "test_input = \"Translate English to French: Hi How are you, whats happening, tell me somthing new?\"\n",
    "output = model.generate(test_input)\n",
    "print(\"Translation:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c49eaab-7e6d-40e9-99cc-31fbeb10ef3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/lib/python3.10/dist-packages/ray/thirdparty_files', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/data/vllm', '/usr/lib/python3/dist-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae40c817-3805-49dc-b6fa-5a2a7951d601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed687a3-84af-4021-ac5b-8cb3ac0b610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of '/data/data': ['lost+found', '.Trash-0', 'data', '.ipynb_checkpoints']\n",
      "Contents of '/data/data/vllm/vllm': ['attention', 'core', 'lora', 'model_executor', '__pycache__', 'sampling_params.py', 'worker', 'test_utils.py', 'distributed', 'config.py', '__init__.py', 'py.typed', 'outputs.py', 'engine', 'executor', 'transformers_utils', '_custom_ops.py', 'usage', 'utils.py', 'entrypoints', 'block.py', 'logger.py', 'spec_decode', 'sequence.py']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List contents of '/data' to confirm the structure\n",
    "print(\"Contents of '/data/data':\", os.listdir('/data'))\n",
    "\n",
    "# Specifically, list contents of '/data/vllm/vllm' to see the nested structure\n",
    "print(\"Contents of '/data/data/vllm/vllm':\", os.listdir('/data/data/vllm/vllm'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db528ec-e0b5-4279-a71f-464ad64ec124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported T5ForVLLM: <class 'vllm.model_executor.models.t5.T5ForVLLM'>\n"
     ]
    }
   ],
   "source": [
    "from vllm.model_executor.models.t5 import T5ForVLLM\n",
    "\n",
    "print(\"Successfully imported T5ForVLLM:\", T5ForVLLM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35d7f7d-143a-438f-b6b0-6b51ab84a150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/data/vllm\n",
      "\u001b[01;34m.\u001b[0m\n",
      "├── CMakeLists.txt\n",
      "├── CONTRIBUTING.md\n",
      "├── Dockerfile\n",
      "├── Dockerfile.cpu\n",
      "├── Dockerfile.rocm\n",
      "├── LICENSE\n",
      "├── MANIFEST.in\n",
      "├── README.md\n",
      "├── \u001b[01;34mbenchmarks\u001b[0m\n",
      "│   ├── README.md\n",
      "│   ├── backend_request_func.py\n",
      "│   ├── benchmark_latency.py\n",
      "│   ├── benchmark_prefix_caching.py\n",
      "│   ├── benchmark_serving.py\n",
      "│   ├── benchmark_throughput.py\n",
      "│   ├── \u001b[01;34mkernels\u001b[0m\n",
      "│   │   ├── benchmark_mixtral_moe.py\n",
      "│   │   ├── benchmark_paged_attention.py\n",
      "│   │   └── benchmark_rope.py\n",
      "│   ├── \u001b[01;32mlaunch_tgi_server.sh\u001b[0m\n",
      "│   └── sonnet.txt\n",
      "├── \u001b[01;34mcmake\u001b[0m\n",
      "│   ├── cpu_extension.cmake\n",
      "│   ├── \u001b[01;32mhipify.py\u001b[0m\n",
      "│   └── utils.cmake\n",
      "├── collect_env.py\n",
      "├── \u001b[01;34mcsrc\u001b[0m\n",
      "│   ├── activation_kernels.cu\n",
      "│   ├── \u001b[01;34mattention\u001b[0m\n",
      "│   │   ├── attention_dtypes.h\n",
      "│   │   ├── attention_generic.cuh\n",
      "│   │   ├── attention_kernels.cu\n",
      "│   │   ├── attention_utils.cuh\n",
      "│   │   ├── dtype_bfloat16.cuh\n",
      "│   │   ├── dtype_float16.cuh\n",
      "│   │   ├── dtype_float32.cuh\n",
      "│   │   └── dtype_fp8.cuh\n",
      "│   ├── cache.h\n",
      "│   ├── cache_kernels.cu\n",
      "│   ├── \u001b[01;34mcpu\u001b[0m\n",
      "│   │   ├── activation.cpp\n",
      "│   │   ├── attention.cpp\n",
      "│   │   ├── cache.cpp\n",
      "│   │   ├── cpu_types.hpp\n",
      "│   │   ├── layernorm.cpp\n",
      "│   │   ├── pos_encoding.cpp\n",
      "│   │   └── pybind.cpp\n",
      "│   ├── cuda_compat.h\n",
      "│   ├── cuda_utils.h\n",
      "│   ├── cuda_utils_kernels.cu\n",
      "│   ├── custom_all_reduce.cu\n",
      "│   ├── custom_all_reduce.cuh\n",
      "│   ├── custom_all_reduce_test.cu\n",
      "│   ├── dispatch_utils.h\n",
      "│   ├── layernorm_kernels.cu\n",
      "│   ├── \u001b[01;34mmoe\u001b[0m\n",
      "│   │   ├── moe_ops.cpp\n",
      "│   │   ├── moe_ops.h\n",
      "│   │   └── topk_softmax_kernels.cu\n",
      "│   ├── moe_align_block_size_kernels.cu\n",
      "│   ├── ops.h\n",
      "│   ├── pos_encoding_kernels.cu\n",
      "│   ├── \u001b[01;34mpunica\u001b[0m\n",
      "│   │   ├── LICENSE\n",
      "│   │   ├── \u001b[01;34mbgmv\u001b[0m\n",
      "│   │   │   ├── bgmv_bf16_bf16_bf16.cu\n",
      "│   │   │   ├── bgmv_bf16_bf16_fp16.cu\n",
      "│   │   │   ├── bgmv_bf16_fp16_bf16.cu\n",
      "│   │   │   ├── bgmv_bf16_fp16_fp16.cu\n",
      "│   │   │   ├── bgmv_bf16_fp32_bf16.cu\n",
      "│   │   │   ├── bgmv_bf16_fp32_fp16.cu\n",
      "│   │   │   ├── bgmv_config.h\n",
      "│   │   │   ├── bgmv_fp16_bf16_bf16.cu\n",
      "│   │   │   ├── bgmv_fp16_bf16_fp16.cu\n",
      "│   │   │   ├── bgmv_fp16_fp16_bf16.cu\n",
      "│   │   │   ├── bgmv_fp16_fp16_fp16.cu\n",
      "│   │   │   ├── bgmv_fp16_fp32_bf16.cu\n",
      "│   │   │   ├── bgmv_fp16_fp32_fp16.cu\n",
      "│   │   │   ├── bgmv_fp32_bf16_bf16.cu\n",
      "│   │   │   ├── bgmv_fp32_bf16_fp16.cu\n",
      "│   │   │   ├── bgmv_fp32_fp16_bf16.cu\n",
      "│   │   │   ├── bgmv_fp32_fp16_fp16.cu\n",
      "│   │   │   ├── bgmv_fp32_fp32_bf16.cu\n",
      "│   │   │   ├── bgmv_fp32_fp32_fp16.cu\n",
      "│   │   │   ├── bgmv_impl.cuh\n",
      "│   │   │   ├── generator.py\n",
      "│   │   │   └── vec_dtypes.cuh\n",
      "│   │   └── punica_ops.cc\n",
      "│   ├── pybind.cpp\n",
      "│   ├── \u001b[01;34mquantization\u001b[0m\n",
      "│   │   ├── \u001b[01;34mawq\u001b[0m\n",
      "│   │   │   ├── dequantize.cuh\n",
      "│   │   │   └── gemm_kernels.cu\n",
      "│   │   ├── \u001b[01;34mfp8\u001b[0m\n",
      "│   │   │   └── \u001b[01;34mamd_detail\u001b[0m\n",
      "│   │   │       ├── hip_float8.h\n",
      "│   │   │       ├── hip_float8_impl.h\n",
      "│   │   │       └── quant_utils.cuh\n",
      "│   │   ├── \u001b[01;34mfp8_e5m2_kvcache\u001b[0m\n",
      "│   │   │   └── quant_utils.cuh\n",
      "│   │   ├── \u001b[01;34mgptq\u001b[0m\n",
      "│   │   │   ├── compat.cuh\n",
      "│   │   │   ├── matrix_view.cuh\n",
      "│   │   │   ├── q_gemm.cu\n",
      "│   │   │   ├── qdq_2.cuh\n",
      "│   │   │   ├── qdq_3.cuh\n",
      "│   │   │   ├── qdq_4.cuh\n",
      "│   │   │   ├── qdq_8.cuh\n",
      "│   │   │   └── qdq_util.cuh\n",
      "│   │   ├── \u001b[01;34mmarlin\u001b[0m\n",
      "│   │   │   ├── LICENSE\n",
      "│   │   │   └── marlin_cuda_kernel.cu\n",
      "│   │   └── \u001b[01;34msqueezellm\u001b[0m\n",
      "│   │       └── quant_cuda_kernel.cu\n",
      "│   └── reduction_utils.cuh\n",
      "├── \u001b[01;34mdocs\u001b[0m\n",
      "│   ├── Makefile\n",
      "│   ├── README.md\n",
      "│   ├── make.bat\n",
      "│   ├── requirements-docs.txt\n",
      "│   └── \u001b[01;34msource\u001b[0m\n",
      "│       ├── \u001b[01;34massets\u001b[0m\n",
      "│       │   ├── \u001b[01;34mkernel\u001b[0m\n",
      "│       │   │   ├── \u001b[01;35mk_vecs.png\u001b[0m\n",
      "│       │   │   ├── \u001b[01;35mkey.png\u001b[0m\n",
      "│       │   │   ├── \u001b[01;35mlogits_vec.png\u001b[0m\n",
      "│       │   │   ├── \u001b[01;35mq_vecs.png\u001b[0m\n",
      "│       │   │   ├── \u001b[01;35mquery.png\u001b[0m\n",
      "│       │   │   ├── \u001b[01;35mv_vec.png\u001b[0m\n",
      "│       │   │   └── \u001b[01;35mvalue.png\u001b[0m\n",
      "│       │   └── \u001b[01;34mlogos\u001b[0m\n",
      "│       │       ├── \u001b[01;35mvllm-logo-only-light.png\u001b[0m\n",
      "│       │       ├── \u001b[01;35mvllm-logo-text-dark.png\u001b[0m\n",
      "│       │       └── \u001b[01;35mvllm-logo-text-light.png\u001b[0m\n",
      "│       ├── conf.py\n",
      "│       ├── \u001b[01;34mdev\u001b[0m\n",
      "│       │   ├── \u001b[01;34mengine\u001b[0m\n",
      "│       │   │   ├── async_llm_engine.rst\n",
      "│       │   │   ├── engine_index.rst\n",
      "│       │   │   └── llm_engine.rst\n",
      "│       │   ├── \u001b[01;34mkernel\u001b[0m\n",
      "│       │   │   └── paged_attention.rst\n",
      "│       │   └── sampling_params.rst\n",
      "│       ├── \u001b[01;34mgetting_started\u001b[0m\n",
      "│       │   ├── amd-installation.rst\n",
      "│       │   ├── cpu-installation.rst\n",
      "│       │   ├── installation.rst\n",
      "│       │   ├── neuron-installation.rst\n",
      "│       │   └── quickstart.rst\n",
      "│       ├── index.rst\n",
      "│       ├── \u001b[01;34mmodels\u001b[0m\n",
      "│       │   ├── adding_model.rst\n",
      "│       │   ├── engine_args.rst\n",
      "│       │   ├── lora.rst\n",
      "│       │   └── supported_models.rst\n",
      "│       ├── \u001b[01;34mquantization\u001b[0m\n",
      "│       │   ├── auto_awq.rst\n",
      "│       │   ├── fp8_e4m3_kvcache.rst\n",
      "│       │   └── fp8_e5m2_kvcache.rst\n",
      "│       └── \u001b[01;34mserving\u001b[0m\n",
      "│           ├── deploying_with_bentoml.rst\n",
      "│           ├── deploying_with_docker.rst\n",
      "│           ├── deploying_with_kserve.rst\n",
      "│           ├── deploying_with_triton.rst\n",
      "│           ├── distributed_serving.rst\n",
      "│           ├── integrations.rst\n",
      "│           ├── metrics.rst\n",
      "│           ├── openai_compatible_server.md\n",
      "│           ├── run_on_sky.rst\n",
      "│           ├── serving_with_langchain.rst\n",
      "│           └── usage_stats.md\n",
      "├── \u001b[01;34mexamples\u001b[0m\n",
      "│   ├── api_client.py\n",
      "│   ├── \u001b[01;34mfp8\u001b[0m\n",
      "│   │   ├── README.md\n",
      "│   │   ├── extract_scales.py\n",
      "│   │   └── \u001b[01;34mquantizer\u001b[0m\n",
      "│   │       ├── README.md\n",
      "│   │       └── quantize.py\n",
      "│   ├── gradio_openai_chatbot_webserver.py\n",
      "│   ├── gradio_webserver.py\n",
      "│   ├── llava_example.py\n",
      "│   ├── llm_engine_example.py\n",
      "│   ├── multilora_inference.py\n",
      "│   ├── offline_inference.py\n",
      "│   ├── offline_inference_distributed.py\n",
      "│   ├── \u001b[01;32moffline_inference_neuron.py\u001b[0m\n",
      "│   ├── offline_inference_with_prefix.py\n",
      "│   ├── openai_chatcompletion_client.py\n",
      "│   ├── openai_completion_client.py\n",
      "│   ├── \u001b[01;34mproduction_monitoring\u001b[0m\n",
      "│   │   ├── README.md\n",
      "│   │   ├── docker-compose.yaml\n",
      "│   │   ├── grafana.json\n",
      "│   │   └── prometheus.yaml\n",
      "│   ├── template_alpaca.jinja\n",
      "│   ├── template_baichuan.jinja\n",
      "│   ├── template_chatglm.jinja\n",
      "│   ├── template_chatglm2.jinja\n",
      "│   ├── template_chatml.jinja\n",
      "│   ├── template_falcon.jinja\n",
      "│   ├── template_falcon_180b.jinja\n",
      "│   ├── template_inkbot.jinja\n",
      "│   └── tensorize_vllm_model.py\n",
      "├── \u001b[01;32mformat.sh\u001b[0m\n",
      "├── patch_xformers.rocm.sh\n",
      "├── pyproject.toml\n",
      "├── requirements-build.txt\n",
      "├── requirements-common.txt\n",
      "├── requirements-cpu.txt\n",
      "├── requirements-cuda.txt\n",
      "├── requirements-dev.txt\n",
      "├── requirements-neuron.txt\n",
      "├── requirements-rocm.txt\n",
      "├── \u001b[01;34mrocm_patch\u001b[0m\n",
      "│   ├── commonpy_xformers-0.0.23.rocm.patch\n",
      "│   ├── flashpy_xformers-0.0.23.rocm.patch\n",
      "│   └── rocm_bf16.patch\n",
      "├── setup.py\n",
      "├── \u001b[01;34mtests\u001b[0m\n",
      "│   ├── __init__.py\n",
      "│   ├── \u001b[01;34masync_engine\u001b[0m\n",
      "│   │   ├── api_server_async_engine.py\n",
      "│   │   ├── test_api_server.py\n",
      "│   │   ├── test_async_llm_engine.py\n",
      "│   │   ├── test_chat_template.py\n",
      "│   │   └── test_request_tracker.py\n",
      "│   ├── \u001b[01;34mbasic_correctness\u001b[0m\n",
      "│   │   ├── test_basic_correctness.py\n",
      "│   │   └── test_chunked_prefill.py\n",
      "│   ├── conftest.py\n",
      "│   ├── \u001b[01;34mcore\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34mblock\u001b[0m\n",
      "│   │   │   ├── __init__.py\n",
      "│   │   │   ├── conftest.py\n",
      "│   │   │   ├── \u001b[01;34me2e\u001b[0m\n",
      "│   │   │   │   ├── conftest.py\n",
      "│   │   │   │   └── test_correctness.py\n",
      "│   │   │   ├── test_block_manager_v2.py\n",
      "│   │   │   ├── test_block_table.py\n",
      "│   │   │   ├── test_common.py\n",
      "│   │   │   ├── test_cpu_gpu_block_allocator.py\n",
      "│   │   │   ├── test_naive_block.py\n",
      "│   │   │   └── test_prefix_caching_block.py\n",
      "│   │   ├── test_block_manager.py\n",
      "│   │   ├── test_chunked_prefill_scheduler.py\n",
      "│   │   ├── test_scheduler.py\n",
      "│   │   └── utils.py\n",
      "│   ├── \u001b[01;34mdistributed\u001b[0m\n",
      "│   │   ├── test_basic_distributed_correctness.py\n",
      "│   │   ├── test_chunked_prefill_distributed.py\n",
      "│   │   ├── test_comm_ops.py\n",
      "│   │   ├── test_custom_all_reduce.py\n",
      "│   │   └── test_pynccl.py\n",
      "│   ├── \u001b[01;34mengine\u001b[0m\n",
      "│   │   ├── test_computed_prefix_blocks.py\n",
      "│   │   ├── test_detokenization.py\n",
      "│   │   ├── test_stop_reason.py\n",
      "│   │   └── test_stop_strings.py\n",
      "│   ├── \u001b[01;34mentrypoints\u001b[0m\n",
      "│   │   ├── test_guided_processors.py\n",
      "│   │   ├── test_openai_server.py\n",
      "│   │   └── test_server_oot_registration.py\n",
      "│   ├── \u001b[01;34mfp8_kv\u001b[0m\n",
      "│   │   ├── \u001b[01;34mllama2-70b-fp8-kv\u001b[0m\n",
      "│   │   │   └── kv_cache_scales.json\n",
      "│   │   └── \u001b[01;34mllama2-7b-fp8-kv\u001b[0m\n",
      "│   │       └── kv_cache_scales.json\n",
      "│   ├── \u001b[01;34mkernels\u001b[0m\n",
      "│   │   ├── allclose_default.py\n",
      "│   │   ├── conftest.py\n",
      "│   │   ├── test_activation.py\n",
      "│   │   ├── test_attention.py\n",
      "│   │   ├── test_cache.py\n",
      "│   │   ├── test_layernorm.py\n",
      "│   │   ├── test_moe.py\n",
      "│   │   ├── test_pos_encoding.py\n",
      "│   │   ├── test_prefix_prefill.py\n",
      "│   │   ├── test_rand.py\n",
      "│   │   └── test_sampler.py\n",
      "│   ├── \u001b[01;34mlora\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── conftest.py\n",
      "│   │   ├── test_baichuan.py\n",
      "│   │   ├── test_chatglm3.py\n",
      "│   │   ├── test_gemma.py\n",
      "│   │   ├── test_layer_variation.py\n",
      "│   │   ├── test_layers.py\n",
      "│   │   ├── test_llama.py\n",
      "│   │   ├── test_lora.py\n",
      "│   │   ├── test_lora_checkpoints.py\n",
      "│   │   ├── test_lora_manager.py\n",
      "│   │   ├── test_mixtral.py\n",
      "│   │   ├── test_punica.py\n",
      "│   │   ├── test_quant_model.py\n",
      "│   │   ├── test_tokenizer_group.py\n",
      "│   │   ├── test_utils.py\n",
      "│   │   ├── test_worker.py\n",
      "│   │   └── utils.py\n",
      "│   ├── \u001b[01;34mmetrics\u001b[0m\n",
      "│   │   └── test_metrics.py\n",
      "│   ├── \u001b[01;34mmodel_executor\u001b[0m\n",
      "│   │   └── weight_utils.py\n",
      "│   ├── \u001b[01;34mmodels\u001b[0m\n",
      "│   │   ├── test_big_models.py\n",
      "│   │   ├── test_llava.py\n",
      "│   │   ├── test_marlin.py\n",
      "│   │   ├── test_mistral.py\n",
      "│   │   ├── test_models.py\n",
      "│   │   └── test_oot_registration.py\n",
      "│   ├── \u001b[01;34mprefix_caching\u001b[0m\n",
      "│   │   └── test_prefix_caching.py\n",
      "│   ├── \u001b[01;34mprompts\u001b[0m\n",
      "│   │   ├── example.txt\n",
      "│   │   └── summary.txt\n",
      "│   ├── \u001b[01;34mquantization\u001b[0m\n",
      "│   │   └── test_autogptq_marlin_configs.py\n",
      "│   ├── \u001b[01;34msamplers\u001b[0m\n",
      "│   │   ├── test_beam_search.py\n",
      "│   │   ├── test_logits_processor.py\n",
      "│   │   ├── test_logprobs.py\n",
      "│   │   ├── test_ranks.py\n",
      "│   │   ├── test_rejection_sampler.py\n",
      "│   │   ├── test_sampler.py\n",
      "│   │   └── test_seeded_generate.py\n",
      "│   ├── \u001b[01;34mspec_decode\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34me2e\u001b[0m\n",
      "│   │   │   ├── conftest.py\n",
      "│   │   │   └── test_correctness.py\n",
      "│   │   ├── test_batch_expansion.py\n",
      "│   │   ├── test_metrics.py\n",
      "│   │   ├── test_multi_step_worker.py\n",
      "│   │   ├── test_spec_decode_worker.py\n",
      "│   │   ├── test_utils.py\n",
      "│   │   └── utils.py\n",
      "│   ├── \u001b[01;34mtensorizer\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── tensorize_vllm_model_for_testing.py\n",
      "│   │   └── test_tensorizer.py\n",
      "│   ├── test_cache_block_hashing.py\n",
      "│   ├── test_config.py\n",
      "│   ├── test_logits_processor.py\n",
      "│   ├── test_regression.py\n",
      "│   ├── test_sampling_params.py\n",
      "│   ├── test_sequence.py\n",
      "│   ├── \u001b[01;34mtokenization\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── test_cached_tokenizer.py\n",
      "│   │   ├── test_detokenize.py\n",
      "│   │   └── test_tokenizer_group.py\n",
      "│   └── \u001b[01;34mworker\u001b[0m\n",
      "│       ├── __init__.py\n",
      "│       ├── test_model_runner.py\n",
      "│       └── test_swap.py\n",
      "├── \u001b[01;34mvllm\u001b[0m\n",
      "│   ├── __init__.py\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   ├── __init__.cpython-310.pyc\n",
      "│   │   ├── _custom_ops.cpython-310.pyc\n",
      "│   │   ├── block.cpython-310.pyc\n",
      "│   │   ├── config.cpython-310.pyc\n",
      "│   │   ├── logger.cpython-310.pyc\n",
      "│   │   ├── outputs.cpython-310.pyc\n",
      "│   │   ├── sampling_params.cpython-310.pyc\n",
      "│   │   ├── sequence.cpython-310.pyc\n",
      "│   │   └── utils.cpython-310.pyc\n",
      "│   ├── _custom_ops.py\n",
      "│   ├── \u001b[01;34mattention\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   ├── layer.cpython-310.pyc\n",
      "│   │   │   └── selector.cpython-310.pyc\n",
      "│   │   ├── \u001b[01;34mbackends\u001b[0m\n",
      "│   │   │   ├── __init__.py\n",
      "│   │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   │   └── abstract.cpython-310.pyc\n",
      "│   │   │   ├── abstract.py\n",
      "│   │   │   ├── flash_attn.py\n",
      "│   │   │   ├── rocm_flash_attn.py\n",
      "│   │   │   ├── torch_sdpa.py\n",
      "│   │   │   └── xformers.py\n",
      "│   │   ├── layer.py\n",
      "│   │   ├── \u001b[01;34mops\u001b[0m\n",
      "│   │   │   ├── __init__.py\n",
      "│   │   │   ├── paged_attn.py\n",
      "│   │   │   ├── prefix_prefill.py\n",
      "│   │   │   └── triton_flash_attention.py\n",
      "│   │   └── selector.py\n",
      "│   ├── block.py\n",
      "│   ├── config.py\n",
      "│   ├── \u001b[01;34mcore\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   ├── interfaces.cpython-310.pyc\n",
      "│   │   │   ├── policy.cpython-310.pyc\n",
      "│   │   │   └── scheduler.cpython-310.pyc\n",
      "│   │   ├── \u001b[01;34mblock\u001b[0m\n",
      "│   │   │   ├── __init__.py\n",
      "│   │   │   ├── block_table.py\n",
      "│   │   │   ├── common.py\n",
      "│   │   │   ├── cpu_gpu_block_allocator.py\n",
      "│   │   │   ├── interfaces.py\n",
      "│   │   │   ├── naive_block.py\n",
      "│   │   │   └── prefix_caching_block.py\n",
      "│   │   ├── block_manager_v1.py\n",
      "│   │   ├── block_manager_v2.py\n",
      "│   │   ├── evictor.py\n",
      "│   │   ├── interfaces.py\n",
      "│   │   ├── policy.py\n",
      "│   │   └── scheduler.py\n",
      "│   ├── \u001b[01;34mdistributed\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   ├── communication_op.cpython-310.pyc\n",
      "│   │   │   ├── parallel_state.cpython-310.pyc\n",
      "│   │   │   └── utils.cpython-310.pyc\n",
      "│   │   ├── communication_op.py\n",
      "│   │   ├── \u001b[01;34mdevice_communicators\u001b[0m\n",
      "│   │   │   ├── __init__.py\n",
      "│   │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   │   ├── custom_all_reduce.cpython-310.pyc\n",
      "│   │   │   │   ├── pynccl.cpython-310.pyc\n",
      "│   │   │   │   └── pynccl_utils.cpython-310.pyc\n",
      "│   │   │   ├── custom_all_reduce.py\n",
      "│   │   │   ├── pynccl.py\n",
      "│   │   │   └── pynccl_utils.py\n",
      "│   │   ├── parallel_state.py\n",
      "│   │   └── utils.py\n",
      "│   ├── \u001b[01;34mengine\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   ├── arg_utils.cpython-310.pyc\n",
      "│   │   │   ├── async_llm_engine.cpython-310.pyc\n",
      "│   │   │   ├── llm_engine.cpython-310.pyc\n",
      "│   │   │   ├── metrics.cpython-310.pyc\n",
      "│   │   │   └── ray_utils.cpython-310.pyc\n",
      "│   │   ├── arg_utils.py\n",
      "│   │   ├── async_llm_engine.py\n",
      "│   │   ├── llm_engine.py\n",
      "│   │   ├── metrics.py\n",
      "│   │   └── ray_utils.py\n",
      "│   ├── \u001b[01;34mentrypoints\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   └── llm.cpython-310.pyc\n",
      "│   │   ├── api_server.py\n",
      "│   │   ├── llm.py\n",
      "│   │   └── \u001b[01;34mopenai\u001b[0m\n",
      "│   │       ├── __init__.py\n",
      "│   │       ├── api_server.py\n",
      "│   │       ├── cli_args.py\n",
      "│   │       ├── protocol.py\n",
      "│   │       ├── serving_chat.py\n",
      "│   │       ├── serving_completion.py\n",
      "│   │       └── serving_engine.py\n",
      "│   ├── \u001b[01;34mexecutor\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   └── executor_base.cpython-310.pyc\n",
      "│   │   ├── cpu_executor.py\n",
      "│   │   ├── executor_base.py\n",
      "│   │   ├── gpu_executor.py\n",
      "│   │   ├── neuron_executor.py\n",
      "│   │   └── ray_gpu_executor.py\n",
      "│   ├── logger.py\n",
      "│   ├── \u001b[01;34mlora\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   ├── layers.cpython-310.pyc\n",
      "│   │   │   ├── lora.cpython-310.pyc\n",
      "│   │   │   ├── models.cpython-310.pyc\n",
      "│   │   │   ├── punica.cpython-310.pyc\n",
      "│   │   │   ├── request.cpython-310.pyc\n",
      "│   │   │   ├── utils.cpython-310.pyc\n",
      "│   │   │   └── worker_manager.cpython-310.pyc\n",
      "│   │   ├── layers.py\n",
      "│   │   ├── lora.py\n",
      "│   │   ├── models.py\n",
      "│   │   ├── punica.py\n",
      "│   │   ├── request.py\n",
      "│   │   ├── utils.py\n",
      "│   │   └── worker_manager.py\n",
      "│   ├── \u001b[01;34mmodel_executor\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   ├── model_loader.cpython-310.pyc\n",
      "│   │   │   ├── sampling_metadata.cpython-310.pyc\n",
      "│   │   │   ├── tensorizer_loader.cpython-310.pyc\n",
      "│   │   │   ├── utils.cpython-310.pyc\n",
      "│   │   │   └── weight_utils.cpython-310.pyc\n",
      "│   │   ├── \u001b[01;34mguided_decoding\u001b[0m\n",
      "│   │   │   ├── __init__.py\n",
      "│   │   │   ├── lm_format_enforcer_decoding.py\n",
      "│   │   │   ├── outlines_decoding.py\n",
      "│   │   │   └── outlines_logits_processors.py\n",
      "│   │   ├── \u001b[01;34mlayers\u001b[0m\n",
      "│   │   │   ├── __init__.py\n",
      "│   │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   │   ├── activation.cpython-310.pyc\n",
      "│   │   │   │   ├── layernorm.cpython-310.pyc\n",
      "│   │   │   │   ├── linear.cpython-310.pyc\n",
      "│   │   │   │   ├── logits_processor.cpython-310.pyc\n",
      "│   │   │   │   ├── rotary_embedding.cpython-310.pyc\n",
      "│   │   │   │   ├── sampler.cpython-310.pyc\n",
      "│   │   │   │   └── vocab_parallel_embedding.cpython-310.pyc\n",
      "│   │   │   ├── activation.py\n",
      "│   │   │   ├── \u001b[01;34mfused_moe\u001b[0m\n",
      "│   │   │   │   ├── __init__.py\n",
      "│   │   │   │   ├── \u001b[01;34mconfigs\u001b[0m\n",
      "│   │   │   │   │   ├── E=16,N=1344,device_name=NVIDIA_A100-SXM4-40GB.json\n",
      "│   │   │   │   │   ├── E=16,N=1344,device_name=NVIDIA_A100-SXM4-80GB.json\n",
      "│   │   │   │   │   ├── E=16,N=1344,device_name=NVIDIA_H100_80GB_HBM3.json\n",
      "│   │   │   │   │   ├── E=16,N=2688,device_name=NVIDIA_A100-SXM4-80GB.json\n",
      "│   │   │   │   │   ├── E=16,N=2688,device_name=NVIDIA_H100_80GB_HBM3.json\n",
      "│   │   │   │   │   ├── E=8,N=1792,device_name=NVIDIA_A100-SXM4-40GB.json\n",
      "│   │   │   │   │   ├── E=8,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json\n",
      "│   │   │   │   │   ├── E=8,N=1792,device_name=NVIDIA_H100_80GB_HBM3.json\n",
      "│   │   │   │   │   ├── E=8,N=2048,device_name=NVIDIA_A100-SXM4-80GB.json\n",
      "│   │   │   │   │   ├── E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3.json\n",
      "│   │   │   │   │   ├── E=8,N=3584,device_name=NVIDIA_A100-SXM4-40GB.json\n",
      "│   │   │   │   │   ├── E=8,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json\n",
      "│   │   │   │   │   ├── E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3.json\n",
      "│   │   │   │   │   ├── E=8,N=4096,device_name=NVIDIA_A100-SXM4-80GB.json\n",
      "│   │   │   │   │   ├── E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3.json\n",
      "│   │   │   │   │   ├── E=8,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json\n",
      "│   │   │   │   │   ├── E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3.json\n",
      "│   │   │   │   │   └── README\n",
      "│   │   │   │   └── fused_moe.py\n",
      "│   │   │   ├── layernorm.py\n",
      "│   │   │   ├── linear.py\n",
      "│   │   │   ├── logits_processor.py\n",
      "│   │   │   ├── \u001b[01;34mops\u001b[0m\n",
      "│   │   │   │   ├── __init__.py\n",
      "│   │   │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   │   │   ├── rand.cpython-310.pyc\n",
      "│   │   │   │   │   └── sample.cpython-310.pyc\n",
      "│   │   │   │   ├── rand.py\n",
      "│   │   │   │   └── sample.py\n",
      "│   │   │   ├── \u001b[01;34mquantization\u001b[0m\n",
      "│   │   │   │   ├── __init__.py\n",
      "│   │   │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   │   │   ├── awq.cpython-310.pyc\n",
      "│   │   │   │   │   ├── base_config.cpython-310.pyc\n",
      "│   │   │   │   │   ├── gptq.cpython-310.pyc\n",
      "│   │   │   │   │   ├── marlin.cpython-310.pyc\n",
      "│   │   │   │   │   ├── schema.cpython-310.pyc\n",
      "│   │   │   │   │   └── squeezellm.cpython-310.pyc\n",
      "│   │   │   │   ├── awq.py\n",
      "│   │   │   │   ├── base_config.py\n",
      "│   │   │   │   ├── gptq.py\n",
      "│   │   │   │   ├── marlin.py\n",
      "│   │   │   │   ├── schema.py\n",
      "│   │   │   │   └── squeezellm.py\n",
      "│   │   │   ├── rejection_sampler.py\n",
      "│   │   │   ├── rotary_embedding.py\n",
      "│   │   │   ├── sampler.py\n",
      "│   │   │   └── vocab_parallel_embedding.py\n",
      "│   │   ├── model_loader.py\n",
      "│   │   ├── \u001b[01;34mmodels\u001b[0m\n",
      "│   │   │   ├── \u001b[01;32m__init__.py\u001b[0m\n",
      "│   │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   │   ├── llama.cpython-310.pyc\n",
      "│   │   │   │   ├── llava.cpython-310.pyc\n",
      "│   │   │   │   └── t5.cpython-310.pyc\n",
      "│   │   │   ├── baichuan.py\n",
      "│   │   │   ├── bloom.py\n",
      "│   │   │   ├── chatglm.py\n",
      "│   │   │   ├── commandr.py\n",
      "│   │   │   ├── dbrx.py\n",
      "│   │   │   ├── decilm.py\n",
      "│   │   │   ├── deepseek.py\n",
      "│   │   │   ├── falcon.py\n",
      "│   │   │   ├── gemma.py\n",
      "│   │   │   ├── gpt2.py\n",
      "│   │   │   ├── gpt_bigcode.py\n",
      "│   │   │   ├── gpt_j.py\n",
      "│   │   │   ├── gpt_neox.py\n",
      "│   │   │   ├── internlm2.py\n",
      "│   │   │   ├── jais.py\n",
      "│   │   │   ├── llama.py\n",
      "│   │   │   ├── llava.py\n",
      "│   │   │   ├── minicpm.py\n",
      "│   │   │   ├── mixtral.py\n",
      "│   │   │   ├── mixtral_quant.py\n",
      "│   │   │   ├── mpt.py\n",
      "│   │   │   ├── olmo.py\n",
      "│   │   │   ├── opt.py\n",
      "│   │   │   ├── orion.py\n",
      "│   │   │   ├── phi.py\n",
      "│   │   │   ├── qwen.py\n",
      "│   │   │   ├── qwen2.py\n",
      "│   │   │   ├── qwen2_moe.py\n",
      "│   │   │   ├── stablelm.py\n",
      "│   │   │   ├── starcoder2.py\n",
      "│   │   │   ├── \u001b[01;34mt5-small-model\u001b[0m\n",
      "│   │   │   │   ├── config.json\n",
      "│   │   │   │   └── model.safetensors\n",
      "│   │   │   ├── \u001b[01;34mt5-small-tokenizer\u001b[0m\n",
      "│   │   │   │   ├── added_tokens.json\n",
      "│   │   │   │   ├── special_tokens_map.json\n",
      "│   │   │   │   ├── spiece.model\n",
      "│   │   │   │   └── tokenizer_config.json\n",
      "│   │   │   ├── t5.py\n",
      "│   │   │   └── xverse.py\n",
      "│   │   ├── neuron_model_loader.py\n",
      "│   │   ├── sampling_metadata.py\n",
      "│   │   ├── tensorizer_loader.py\n",
      "│   │   ├── utils.py\n",
      "│   │   └── weight_utils.py\n",
      "│   ├── outputs.py\n",
      "│   ├── py.typed\n",
      "│   ├── sampling_params.py\n",
      "│   ├── sequence.py\n",
      "│   ├── \u001b[01;34mspec_decode\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── batch_expansion.py\n",
      "│   │   ├── interfaces.py\n",
      "│   │   ├── metrics.py\n",
      "│   │   ├── multi_step_worker.py\n",
      "│   │   ├── spec_decode_worker.py\n",
      "│   │   └── util.py\n",
      "│   ├── test_utils.py\n",
      "│   ├── \u001b[01;34mtransformers_utils\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   ├── config.cpython-310.pyc\n",
      "│   │   │   ├── detokenizer.cpython-310.pyc\n",
      "│   │   │   └── tokenizer.cpython-310.pyc\n",
      "│   │   ├── config.py\n",
      "│   │   ├── \u001b[01;34mconfigs\u001b[0m\n",
      "│   │   │   ├── __init__.py\n",
      "│   │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   │   ├── chatglm.cpython-310.pyc\n",
      "│   │   │   │   ├── dbrx.cpython-310.pyc\n",
      "│   │   │   │   ├── falcon.cpython-310.pyc\n",
      "│   │   │   │   ├── jais.cpython-310.pyc\n",
      "│   │   │   │   └── mpt.cpython-310.pyc\n",
      "│   │   │   ├── chatglm.py\n",
      "│   │   │   ├── dbrx.py\n",
      "│   │   │   ├── falcon.py\n",
      "│   │   │   ├── jais.py\n",
      "│   │   │   └── mpt.py\n",
      "│   │   ├── detokenizer.py\n",
      "│   │   ├── tokenizer.py\n",
      "│   │   ├── \u001b[01;34mtokenizer_group\u001b[0m\n",
      "│   │   │   ├── __init__.py\n",
      "│   │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   │   ├── base_tokenizer_group.cpython-310.pyc\n",
      "│   │   │   │   ├── ray_tokenizer_group.cpython-310.pyc\n",
      "│   │   │   │   └── tokenizer_group.cpython-310.pyc\n",
      "│   │   │   ├── base_tokenizer_group.py\n",
      "│   │   │   ├── ray_tokenizer_group.py\n",
      "│   │   │   └── tokenizer_group.py\n",
      "│   │   └── \u001b[01;34mtokenizers\u001b[0m\n",
      "│   │       ├── __init__.py\n",
      "│   │       ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │       │   ├── __init__.cpython-310.pyc\n",
      "│   │       │   └── baichuan.cpython-310.pyc\n",
      "│   │       └── baichuan.py\n",
      "│   ├── \u001b[01;34musage\u001b[0m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   │   │   ├── __init__.cpython-310.pyc\n",
      "│   │   │   └── usage_lib.cpython-310.pyc\n",
      "│   │   └── usage_lib.py\n",
      "│   ├── utils.py\n",
      "│   └── \u001b[01;34mworker\u001b[0m\n",
      "│       ├── __init__.py\n",
      "│       ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│       │   ├── __init__.cpython-310.pyc\n",
      "│       │   ├── cache_engine.cpython-310.pyc\n",
      "│       │   ├── model_runner.cpython-310.pyc\n",
      "│       │   ├── worker.cpython-310.pyc\n",
      "│       │   └── worker_base.cpython-310.pyc\n",
      "│       ├── cache_engine.py\n",
      "│       ├── cpu_model_runner.py\n",
      "│       ├── cpu_worker.py\n",
      "│       ├── model_runner.py\n",
      "│       ├── neuron_model_runner.py\n",
      "│       ├── neuron_worker.py\n",
      "│       ├── worker.py\n",
      "│       └── worker_base.py\n",
      "└── vllm_test.ipynb\n",
      "\n",
      "111 directories, 591 files\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba5168-668f-4bdd-9a58-66eb405d48a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c26dd9d-c664-4079-be85-714fbde58318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.model_executor.models.t5 import T5ForVLLM  # Import your T5 wrapper class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e00cec4-a4c5-4483-a862-c9fd4745fdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from vllm.model_executor.models.t5 import T5ForVLLM  # Adjust the import path according to your project structure\n",
    "\n",
    "# Instantiate the model\n",
    "model_dir = '/data/data/vllm/vllm/model_executor/models/t5-small-model'  # Make sure to provide the correct path to your model files\n",
    "tokenizer_dir = '/data/data/vllm/vllm/model_executor/models/t5-small-tokenizer'  # And the tokenizer files\n",
    "t5_model = T5ForVLLM(model_dir=model_dir, tokenizer_dir=tokenizer_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "780554e6-9ebc-496f-8e55-2d72ed816479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForVLLM(\n",
       "  (model): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32128, 512)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-5): 5 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-5): 5 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "566dd75f-4e22-4973-8b80-c6850ac902a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Text: ['fromage']\n"
     ]
    }
   ],
   "source": [
    "# Assuming your model's `generate` method is set up to take text input directly\n",
    "input_text = \"Translate English to French: cheese\"\n",
    "output_text = t5_model.generate(input_text)\n",
    "print(\"Output Text:\", output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad98257-5bb5-4288-98ca-8d359c90d706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7708a98-8c01-493b-b821-66c8a1519c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67a0fd24-6122-4976-ae13-7b064e2ae155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Assuming T5ForVLLM is imported and model paths are set\n",
    "model = T5ForVLLM(model_dir=model_dir, tokenizer_dir=tokenizer_dir)\n",
    "model.model.eval()  # Make sure the model is in eval mode\n",
    "\n",
    "# Load tokenizer and prepare inputs\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_dir)\n",
    "text = \"Translate English to French: cheese\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "input_ids = encoded_input['input_ids']\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(10):\n",
    "    _ = model.generate(input_text=text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d44e3c-5d5e-4bf6-959f-289e9690f316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b853f8f5-d994-4f09-a3b4-7851929be40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inference Time: 0.207557 seconds per run\n"
     ]
    }
   ],
   "source": [
    "# Timing inference\n",
    "start_time = time.time()\n",
    "num_runs = 100\n",
    "for _ in range(num_runs):\n",
    "    output = model.generate(input_text=text)\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "average_time_per_inference = total_time / num_runs\n",
    "print(f\"Average Inference Time: {average_time_per_inference:.6f} seconds per run\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "340dfebc-b6fa-4d3e-8305-31adab31a3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 4.82 texts per second\n"
     ]
    }
   ],
   "source": [
    "throughput = num_runs / total_time\n",
    "print(f\"Throughput: {throughput:.2f} texts per second\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b1d921b-d8c8-4d11-861f-d28d9ca538fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU usage before: 1.9%, after: 4.0%\n",
      "Memory usage before: 44.75 GB, after: 44.75 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# CPU and Memory Usage before running inference\n",
    "cpu_before = psutil.cpu_percent()\n",
    "mem_before = psutil.virtual_memory().used\n",
    "\n",
    "# Running inference as above\n",
    "# [Insert the inference code here]\n",
    "\n",
    "# CPU and Memory Usage after running inference\n",
    "cpu_after = psutil.cpu_percent()\n",
    "mem_after = psutil.virtual_memory().used\n",
    "\n",
    "print(f\"CPU usage before: {cpu_before}%, after: {cpu_after}%\")\n",
    "print(f\"Memory usage before: {mem_before / (1024**3):.2f} GB, after: {mem_after / (1024**3):.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11584558-89a5-4bda-a093-41b2d6eec454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c54c6b-b68f-4eda-9740-d5e8bf8159a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415ae9ae-bc18-41b2-9e6f-3be6e8b52976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-25 13:23:19 pynccl.py:58] Loading nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 13:23:19,716\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from vllm.model_executor.models.t5 import T5ForVLLM  # Adjust the import path according to your project structure\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Instantiate the model\n",
    "model_dir = '/data/data/vllm/vllm/model_executor/models/t5-small-model'  # Make sure to provide the correct path to your model files\n",
    "tokenizer_dir = '/data/data/vllm/vllm/model_executor/models/t5-small-tokenizer'  # And the tokenizer files\n",
    "# Assuming T5ForVLLM is imported and model paths are set\n",
    "model = T5ForVLLM(model_dir=model_dir, tokenizer_dir=tokenizer_dir)\n",
    "model.model.eval()  # Make sure the model is in eval mode\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75abf290-60a3-4122-b282-62275af2f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, input_text):\n",
    "    # Tokenize the text for input to the model\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    # Generate output tokens\n",
    "    output_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b929e3-07d6-484d-b85d-2b7d6be7b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_text):\n",
    "    # Generate output tokens directly using the adjusted model's generate method\n",
    "    outputs = model.generate(input_text)\n",
    "    return \" \".join(outputs)  # Join all generated outputs into a single string\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c13b81-92de-48af-95ea-28821e3e64e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Wie sind Sie?\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Translate English to Spanish: How are you?\"\n",
    "output_text = generate_text(model, input_text)\n",
    "print(\"Output:\", output_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d0ff4e-9de6-483f-9fbd-9a7500e4064c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary\n\u001b[1;32m      7\u001b[0m document \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    The T5 model is capable of handling a wide array of tasks. This flexibility is due to its text-to-text approach,\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m    which makes it suitable for any task that can be formulated as converting one type of text into another. This includes\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    summarization, where the goal is to produce a concise and accurate summary from a longer text.\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\u001b[39m, summary)\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36msummarize_text\u001b[0;34m(text, max_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_text\u001b[39m(text, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m):\n\u001b[1;32m      2\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarize: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     summary_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(summary_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/models/t5.py:40\u001b[0m, in \u001b[0;36mT5ForVLLM.generate\u001b[0;34m(self, input_text, max_length, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer is not loaded. Cannot generate text without tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Encode the input text\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     42\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2858\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2858\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2860\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2916\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2913\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2916\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2917\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2919\u001b[0m     )\n\u001b[1;32m   2921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2922\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2923\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2924\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2925\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "def summarize_text(text, max_length=150):\n",
    "    input_ids = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    summary_ids = model.generate(input_ids, max_length=max_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "document = \"\"\"\n",
    "    The T5 model is capable of handling a wide array of tasks. This flexibility is due to its text-to-text approach,\n",
    "    which makes it suitable for any task that can be formulated as converting one type of text into another. This includes\n",
    "    summarization, where the goal is to produce a concise and accurate summary from a longer text.\n",
    "    \"\"\"\n",
    "summary = summarize_text(document)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77fbd2-2df0-4330-b8ca-01a2eb82e2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e52f6691-bb6d-418f-9087-06241bc32cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ['Comment êtes-vous?']\n"
     ]
    }
   ],
   "source": [
    "# # Example instantiation and usage\n",
    "# model_dir = '/path/to/t5-small-model'\n",
    "# tokenizer_dir = '/path/to/t5-small-tokenizer'  # Can be None if the tokenizer is stored with the model\n",
    "t5_model = T5ForVLLM(model_dir, tokenizer_dir)\n",
    "\n",
    "input_text = \"Translate English to French: How are you?\"\n",
    "# Now you can pass additional generation parameters such as length_penalty\n",
    "output_texts = t5_model.generate(input_text, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "print(\"Output:\", output_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f6ee86f-004a-4df4-8978-203fcf8a369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(input_text, max_length=150, num_beams=4):\n",
    "    # Prefix the input with \"summarize:\" and encode it\n",
    "    input_text = \"summarize: \" + input_text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Generate summary with beam search\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=num_beams, max_length=max_length, early_stopping=True)\n",
    "    \n",
    "    # Decode the output tensor to a summary string\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da1004e7-907b-427b-ba1a-627d39d585a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mThe quick brown fox jumps over the lazy dog. This famous sentence contains every letter of the alphabet, making it a pangram. \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mUsed since the 1800s, it has been a standard test phrase for typewriters and computer keyboards. The sentence is often used to \u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mtest out fonts because it contains all the letters of the English alphabet.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Call the summarization function\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\u001b[39m, summary)\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36msummarize_text\u001b[0;34m(input_text, max_length, num_beams)\u001b[0m\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Generate summary with beam search\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m summary_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Decode the output tensor to a summary string\u001b[39;00m\n\u001b[1;32m     10\u001b[0m summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(summary_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/models/t5.py:40\u001b[0m, in \u001b[0;36mT5ForVLLM.generate\u001b[0;34m(self, input_text, max_length, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer is not loaded. Cannot generate text without tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Encode the input text\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     42\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2858\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2858\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2860\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2916\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2913\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2916\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2917\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2919\u001b[0m     )\n\u001b[1;32m   2921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2922\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2923\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2924\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2925\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. This famous sentence contains every letter of the alphabet, making it a pangram. \n",
    "Used since the 1800s, it has been a standard test phrase for typewriters and computer keyboards. The sentence is often used to \n",
    "test out fonts because it contains all the letters of the English alphabet.\n",
    "\"\"\"\n",
    "\n",
    "# Call the summarization function\n",
    "summary = summarize_text(sample_text)\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1659a-68fe-4e01-b50c-d002c67399ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data/data/vllm/vllm/model_executor/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7cfe130-63e3-463f-9630-ad8d37d0862f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-26 12:09:03 pynccl.py:58] Loading nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm.model_executor.models.t5_kv_cache.t5_kv_cache'; 'vllm.model_executor.models.t5_kv_cache' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Config\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5_kv_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5WithKVCache  \u001b[38;5;66;03m# adjust 'your_module_path' to where your custom code resides\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example instantiation and use\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/data/data/vllm/vllm/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"vLLM: a high-throughput and memory-efficient inference engine for LLMs\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncEngineArgs, EngineArgs\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_llm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncLLMEngine\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMEngine\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mray_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_ray_cluster\n",
      "File \u001b[0;32m/data/data/vllm/vllm/engine/async_llm_engine.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelConfig\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncEngineArgs\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMEngine\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mray_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_ray_cluster, ray\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "File \u001b[0;32m/data/data/vllm/vllm/engine/llm_engine.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EngineArgs\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StatLogger, Stats\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mray_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_ray_cluster\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExecutorBase\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "File \u001b[0;32m/data/data/vllm/vllm/engine/ray_utils.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_ip, is_hip, set_cuda_visible_devices\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Worker\n\u001b[1;32m      9\u001b[0m logger \u001b[38;5;241m=\u001b[39m init_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/data/vllm/vllm/worker/worker.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SamplerOutput, SequenceGroupMetadata\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CacheEngine\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_runner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelRunner\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WorkerBase\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWorker\u001b[39;00m(WorkerBase):\n",
      "File \u001b[0;32m/data/data/vllm/vllm/worker/model_runner.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRUCacheWorkerLoRAManager\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SamplingMetadata\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling_params\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SamplingParams, SamplingType\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (MultiModalData, SamplerOutput, SequenceData,\n\u001b[1;32m     26\u001b[0m                            SequenceGroupMetadata)\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/model_loader.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceConfig, ModelConfig\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelRegistry\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlavaForConditionalGeneration\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorizer_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     ParameterizedLoadFormat, is_vllm_serialized_tensorizer,\n\u001b[1;32m     13\u001b[0m     load_with_tensorizer)\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/models/__init__.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Optional, Type\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5ForVLLM\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5_kv_cache\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5_kv_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5WithKVCache\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vllm.model_executor.models.t5_kv_cache.t5_kv_cache'; 'vllm.model_executor.models.t5_kv_cache' is not a package"
     ]
    }
   ],
   "source": [
    "from transformers import T5Config\n",
    "from vllm.model_executor.models.t5_kv_cache import T5WithKVCache  # adjust 'your_module_path' to where your custom code resides\n",
    "from vllm.model_executor.models.t5 import T5ForVLLM  # Adjust the import path according to your project structure\n",
    "\n",
    "# Example instantiation and use\n",
    "if __name__ == \"__main__\":\n",
    "    model = T5WithKVCache(\n",
    "        config=T5Config.from_pretrained('/path/to/vllm/model_executor/models/t5-small-model'),\n",
    "        model_dir='/path/to/vllm/model_executor/models/t5-small-model',\n",
    "        tokenizer_dir='/path/to/vllm/model_executor/models/t5-small-tokenizer'\n",
    "    )\n",
    "    print(\"Model successfully created with KV caching.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c46751-a964-45fe-b550-dbc7cb16a035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8993e8-8f34-4325-bb40-1e351479f311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-26 13:12:25 pynccl.py:58] Loading nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm.model_executor.models.t5_kv_cache.t5_kv_cache'; 'vllm.model_executor.models.t5_kv_cache' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5_kv_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5WithKVCache  \u001b[38;5;66;03m# adjust 'your_module_path' to where your custom code resides\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/data/vllm/vllm/model_executor/models/t5-small-model\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Make sure to provide the correct path to your model files\u001b[39;00m\n",
      "File \u001b[0;32m/data/data/vllm/vllm/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"vLLM: a high-throughput and memory-efficient inference engine for LLMs\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncEngineArgs, EngineArgs\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_llm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncLLMEngine\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMEngine\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mray_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_ray_cluster\n",
      "File \u001b[0;32m/data/data/vllm/vllm/engine/async_llm_engine.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelConfig\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncEngineArgs\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMEngine\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mray_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_ray_cluster, ray\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "File \u001b[0;32m/data/data/vllm/vllm/engine/llm_engine.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EngineArgs\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StatLogger, Stats\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mray_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_ray_cluster\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExecutorBase\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "File \u001b[0;32m/data/data/vllm/vllm/engine/ray_utils.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_ip, is_hip, set_cuda_visible_devices\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Worker\n\u001b[1;32m      9\u001b[0m logger \u001b[38;5;241m=\u001b[39m init_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/data/vllm/vllm/worker/worker.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SamplerOutput, SequenceGroupMetadata\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CacheEngine\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_runner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelRunner\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WorkerBase\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWorker\u001b[39;00m(WorkerBase):\n",
      "File \u001b[0;32m/data/data/vllm/vllm/worker/model_runner.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRUCacheWorkerLoRAManager\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SamplingMetadata\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling_params\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SamplingParams, SamplingType\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (MultiModalData, SamplerOutput, SequenceData,\n\u001b[1;32m     26\u001b[0m                            SequenceGroupMetadata)\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/model_loader.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceConfig, ModelConfig\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelRegistry\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlavaForConditionalGeneration\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorizer_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     ParameterizedLoadFormat, is_vllm_serialized_tensorizer,\n\u001b[1;32m     13\u001b[0m     load_with_tensorizer)\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/models/__init__.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Optional, Type\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5ForVLLM\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5_kv_cache\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5_kv_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5WithKVCache\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vllm.model_executor.models.t5_kv_cache.t5_kv_cache'; 'vllm.model_executor.models.t5_kv_cache' is not a package"
     ]
    }
   ],
   "source": [
    "from vllm.model_executor.models.t5_kv_cache import T5WithKVCache  # adjust 'your_module_path' to where your custom code resides\n",
    "\n",
    "# Instantiate the model\n",
    "model_dir = '/data/data/vllm/vllm/model_executor/models/t5-small-model'  # Make sure to provide the correct path to your model files\n",
    "tokenizer_dir = '/data/data/vllm/vllm/model_executor/models/t5-small-tokenizer'  # And the tokenizer files\n",
    "# Assuming T5ForVLLM is imported and model paths are set\n",
    "model = T5WithKVCache(model_dir=model_dir, tokenizer_dir=tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6571cb13-4aef-47a5-a3c0-a58561f61cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5WithKVCache(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0-5): 6 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5AttentionWithCache(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0-5): 6 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5AttentionWithCache(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03189129-18b6-4fe5-bbe6-6bca111b24c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/data/vllm'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eff259d-56c4-404f-97da-78f238b7c66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_name_or_path': 't5-small', 'architectures': ['T5Model'], 'classifier_dropout': 0.0, 'd_ff': 2048, 'd_kv': 64, 'd_model': 512, 'decoder_start_token_id': 0, 'dense_act_fn': 'relu', 'dropout_rate': 0.1, 'eos_token_id': 1, 'feed_forward_proj': 'relu', 'initializer_factor': 1.0, 'is_encoder_decoder': True, 'is_gated_act': False, 'layer_norm_epsilon': 1e-06, 'model_type': 't5', 'n_positions': 512, 'num_decoder_layers': 6, 'num_heads': 8, 'num_layers': 6, 'output_past': True, 'pad_token_id': 0, 'relative_attention_max_distance': 128, 'relative_attention_num_buckets': 32, 'task_specific_params': {'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 200, 'min_length': 30, 'no_repeat_ngram_size': 3, 'num_beams': 4, 'prefix': 'summarize: '}, 'translation_en_to_de': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to German: '}, 'translation_en_to_fr': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to French: '}, 'translation_en_to_ro': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to Romanian: '}}, 'torch_dtype': 'float32', 'transformers_version': '4.40.1', 'use_cache': True, 'vocab_size': 32128}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Specify the file path\n",
    "file_path = 'vllm/model_executor/models/t5-small-model/config.json'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Now 'data' contains the contents of the JSON file\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4122c23-65a8-4991-a612-9a7e2922c54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cef99dca-8cf3-41a9-b922-81085a372612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/data/vllm'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d219e131-a490-4433-88d6-0c8b0e6892f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cache_dir\"]= \"/data/data/vllm/vllm/model_executor/models/t5-small-model\"\n",
    "data[\"use_kv_cache\"]= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f523bc8-57f5-4a30-a71c-d0908a845fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_name_or_path': 't5-small',\n",
       " 'architectures': ['T5Model'],\n",
       " 'classifier_dropout': 0.0,\n",
       " 'd_ff': 2048,\n",
       " 'd_kv': 64,\n",
       " 'd_model': 512,\n",
       " 'decoder_start_token_id': 0,\n",
       " 'dense_act_fn': 'relu',\n",
       " 'dropout_rate': 0.1,\n",
       " 'eos_token_id': 1,\n",
       " 'feed_forward_proj': 'relu',\n",
       " 'initializer_factor': 1.0,\n",
       " 'is_encoder_decoder': True,\n",
       " 'is_gated_act': False,\n",
       " 'layer_norm_epsilon': 1e-06,\n",
       " 'model_type': 't5',\n",
       " 'n_positions': 512,\n",
       " 'num_decoder_layers': 6,\n",
       " 'num_heads': 8,\n",
       " 'num_layers': 6,\n",
       " 'output_past': True,\n",
       " 'pad_token_id': 0,\n",
       " 'relative_attention_max_distance': 128,\n",
       " 'relative_attention_num_buckets': 32,\n",
       " 'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "   'length_penalty': 2.0,\n",
       "   'max_length': 200,\n",
       "   'min_length': 30,\n",
       "   'no_repeat_ngram_size': 3,\n",
       "   'num_beams': 4,\n",
       "   'prefix': 'summarize: '},\n",
       "  'translation_en_to_de': {'early_stopping': True,\n",
       "   'max_length': 300,\n",
       "   'num_beams': 4,\n",
       "   'prefix': 'translate English to German: '},\n",
       "  'translation_en_to_fr': {'early_stopping': True,\n",
       "   'max_length': 300,\n",
       "   'num_beams': 4,\n",
       "   'prefix': 'translate English to French: '},\n",
       "  'translation_en_to_ro': {'early_stopping': True,\n",
       "   'max_length': 300,\n",
       "   'num_beams': 4,\n",
       "   'prefix': 'translate English to Romanian: '}},\n",
       " 'torch_dtype': 'float32',\n",
       " 'transformers_version': '4.40.1',\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 32128,\n",
       " 'cache_dir': '/data/data/vllm/vllm/model_executor/models/t5-small-model',\n",
       " 'use_kv_cache': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a93b4f2-397e-4d85-a5f2-3aea518f8cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec56a7b0-a086-40d5-a904-2ae22d2d6814",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "T5AttentionWithCache.forward() got an unexpected keyword argument 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Ensure the model is in evaluation mode\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient calculation for inference\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m decoded_output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoded Output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, decoded_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1452\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1445\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1446\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration results, please set `padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` when initializing the tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1447\u001b[0m         )\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1452\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:505\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    503\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    504\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 505\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1109\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1095\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1096\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         output_attentions,\n\u001b[1;32m   1107\u001b[0m     )\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:689\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    699\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:596\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    587\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    594\u001b[0m ):\n\u001b[1;32m    595\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 596\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    606\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: T5AttentionWithCache.forward() got an unexpected keyword argument 'mask'"
     ]
    }
   ],
   "source": [
    "# test_t5_kv_cache.py\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Make sure the custom model class is visible here, might require adjusting PYTHONPATH\n",
    "# from vllm.model_executor.models import T5WithKVCache\n",
    "from vllm.model_executor.models.t5_kv_cache import T5WithKVCache \n",
    "# Adjust the path according to where your model and tokenizer are actually saved\n",
    "model_path = 'vllm/model_executor/models/t5-small-model'\n",
    "tokenizer_path = 'vllm/model_executor/models/t5-small-tokenizer'\n",
    "\n",
    "# Load model and tokenizer\n",
    "# model = T5WithKVCache.from_pretrained(model_path)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\n",
    "model = T5WithKVCache(model_dir=model_path, tokenizer_dir=tokenizer_path)\n",
    "\n",
    "# Prepare input for testing\n",
    "input_text = \"Translate English to French: How are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Use the model\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    output_ids = model.generate(inputs['input_ids'], max_length=50, use_cache=True)\n",
    "\n",
    "decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Decoded Output:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb165ecb-9c1f-4cd0-8cde-08c72b4cf550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de969595-46f1-4037-bd7b-70cc87f5924e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d229be2-3149-43f4-8f36-529ccd976589",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T5Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvllm/model_executor/models/t5-small-model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m model \u001b[38;5;241m=\u001b[39m T5WithKVCache(model_dir\u001b[38;5;241m=\u001b[39mmodel_path)\n\u001b[0;32m---> 54\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m     56\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslate English to French: How are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T5Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Config\n",
    "from transformers.models.t5.modeling_t5 import T5Attention\n",
    "\n",
    "class T5AttentionWithCache(T5Attention):\n",
    "    \"\"\"Custom T5 Attention module with Key-Value caching.\"\"\"\n",
    "    def forward(self, hidden_states, attention_mask=None, position_bias=None, \n",
    "                layer_head_mask=None, past_key_value=None, use_cache=False, \n",
    "                output_attentions=False, **kwargs):\n",
    "        # Handle cache if past key values are given\n",
    "        if past_key_value is not None:\n",
    "            past_key, past_value = past_key_value\n",
    "            key_layer = torch.cat([past_key, self.k(hidden_states)], dim=2)\n",
    "            value_layer = torch.cat([past_value, self.v(hidden_states)], dim=2)\n",
    "        else:\n",
    "            key_layer = self.k(hidden_states)\n",
    "            value_layer = self.v(hidden_states)\n",
    "\n",
    "        # Cache the current key and value layers for future use if caching is enabled\n",
    "        if use_cache:\n",
    "            past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # Standard T5 attention calculation\n",
    "        query_layer = self.q(hidden_states)\n",
    "        key_layer = self._shape(key_layer, -1)\n",
    "        value_layer = self._shape(value_layer, -1)\n",
    "        scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        if attention_mask is not None:\n",
    "            scores += attention_mask\n",
    "        attention_probs = torch.nn.Softmax(dim=-1)(scores)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        # Adjust outputs based on whether attention probabilities are also requested\n",
    "        outputs = (context_layer, past_key_value) if use_cache else (context_layer,)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (attention_probs,)\n",
    "        return outputs\n",
    "\n",
    "class T5WithKVCache(T5ForConditionalGeneration):\n",
    "    \"\"\"T5 model with integrated Key-Value caching in its attention layers.\"\"\"\n",
    "    def __init__(self, model_dir, *args, **kwargs):\n",
    "        super().__init__(T5Config.from_pretrained(model_dir), *args, **kwargs)\n",
    "        # Replace all attention layers in the encoder and decoder with KV-cached versions\n",
    "        for block in self.encoder.block:\n",
    "            block.layer[0].SelfAttention = T5AttentionWithCache(self.config)\n",
    "        for block in self.decoder.block:\n",
    "            block.layer[0].SelfAttention = T5AttentionWithCache(self.config)\n",
    "\n",
    "# Example usage if this script is run as the main module\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = 'vllm/model_executor/models/t5-small-model'\n",
    "    \n",
    "    model = T5WithKVCache(model_dir=model_path)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "    input_text = \"Translate English to French: How are you?\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(inputs['input_ids'], max_length=50, use_cache=True)\n",
    "    decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(\"Decoded Output:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad09495-957b-4d1d-9e5e-8cac2ceb8acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4486f9f-e8f5-4845-9407-c0ae653e2fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa3d83-835a-4bca-8be7-027006dafd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "📂 models\n",
    "\n",
    "  📄 deepseek.py\n",
    "  📄 t5_vllm_optimized_cache_1.py\n",
    "  📄 baichuan.py\n",
    "  📄 qwen2_moe.py\n",
    "  📄 orion.py\n",
    "  📄 t5_kv_cache.py\n",
    "  📄 __init__.py\n",
    "  📄 gpt2.py\n",
    "  📄 llama.py\n",
    "  📄 commandr.py\n",
    "  📂 llama_7b\n",
    "    📄 inference.py\n",
    "    📂 tokenizer\n",
    "      📄 tokenizer_checklist.chk\n",
    "      📄 tokenizer.model\n",
    "    📄 llama_for_vllm.py\n",
    "    📂 .ipynb_checkpoints\n",
    "      📄 llama_for_vllm-checkpoint.py\n",
    "      📄 model-checkpoint.py\n",
    "    📂 checkpoints\n",
    "      📄 checklist.chk\n",
    "      📄 consolidated.00.pth\n",
    "      📄 params.json\n",
    "    📄 model.py\n",
    "  📄 decilm.py\n",
    "  📄 opt.py\n",
    "  📄 mixtral.py\n",
    "  📄 xverse.py\n",
    "  📂 t5-small-model\n",
    "    📄 config.json\n",
    "    📂 .ipynb_checkpoints\n",
    "      📄 config-checkpoint.json\n",
    "    📄 model.safetensors\n",
    "  📄 t5_vllm_optimized_cache.py\n",
    "  📄 falcon.py\n",
    "  📂 t5-small-tokenizer\n",
    "    📄 spiece.model\n",
    "    📄 tokenizer_config.json\n",
    "    📄 special_tokens_map.json\n",
    "    📄 added_tokens.json\n",
    "  📄 gemma.py\n",
    "  📄 dbrx.py\n",
    "  📄 minicpm.py\n",
    "  📂 .ipynb_checkpoints\n",
    "    📄 t5-checkpoint.py\n",
    "    📄 llama-checkpoint.py\n",
    "    📄 t5_vllm_optimized_cache-checkpoint.py\n",
    "    📄 t5_vllm_optimized_cache_1-checkpoint.py\n",
    "    📄 __init__-checkpoint.py\n",
    "    📄 t5_kv_cache-checkpoint.py\n",
    "  📄 jais.py\n",
    "  📄 mpt.py\n",
    "  📄 chatglm.py\n",
    "  📄 phi.py\n",
    "  📄 t5.py\n",
    "  📄 stablelm.py\n",
    "  📄 internlm2.py\n",
    "  📄 llava.py\n",
    "  📄 bloom.py\n",
    "  📄 olmo.py\n",
    "  📄 gpt_bigcode.py\n",
    "  📄 qwen2.py\n",
    "  📄 starcoder2.py\n",
    "  📄 qwen.py\n",
    "  📄 gpt_j.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
