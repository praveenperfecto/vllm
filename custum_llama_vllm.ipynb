{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5ec27-9eea-4d15-bba2-f393330df804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d29bc4b-2392-47fe-bd81-1659dd4e4844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/lib/python3.10/dist-packages/ray/thirdparty_files', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/local/lib/python3.10/dist-packages/xformers-0.0.25-py3.10-linux-x86_64.egg', '/usr/lib/python3/dist-packages', '/data/data/vllm', '/tmp/tmp7gb5kqtl']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4614af96-797e-4aed-a6c7-9e02778113f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/data/vllm/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f89f1a1-d6f7-48e2-ba59-4951db169b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vllm.model_executor.models.llama_for_vllm import LLaMAForVLLM  # Ensure this is correctly referenced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c569e1b8-3ec0-4853-b97a-93f07074ef3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cada856c-44d7-4ac0-b174-e711f367dbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/data/vllm'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc6689-ffed-4627-b6ba-f1f64d5c6740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from vllm.model_executor.models.llama_for_vllm import LLaMAForVLLM\n",
    "\n",
    "# Define model and tokenizer paths\n",
    "model_dir = '/data/data/vllm/vllm/model_executor/models/cus_llama_7b/checkpoints/'\n",
    "tokenizer_dir = '/data/data/vllm/vllm/model_executor/models/cus_llama_7b/tokenizer/tokenizer.model'\n",
    "\n",
    "# Initialize the LLaMA tokenizer\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load(tokenizer_dir)\n",
    "\n",
    "# Initialize the LLaMA 7B model\n",
    "model = LLaMAForVLLM(model_dir, tokenizer_dir, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Input text for testing\n",
    "input_text = \"Translate English to French: Hi How are you, what's happening, tell me something new?\"\n",
    "\n",
    "# Function to benchmark a model\n",
    "def benchmark_model(model, text, iterations=5):\n",
    "    # Warm up model\n",
    "    for _ in range(2):\n",
    "        model.generate(text)\n",
    "\n",
    "    # Measure latency and time to first token\n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        start = time.time()\n",
    "        model.generate(text)\n",
    "        first_token_time = time.time() - start  # This simplifies first token time to full generation time\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    avg_latency = total_time / iterations\n",
    "    avg_first_token_time = first_token_time  # Simplistic assumption\n",
    "\n",
    "    # Throughput calculation\n",
    "    throughput = iterations / total_time\n",
    "\n",
    "    return avg_latency, throughput, avg_first_token_time\n",
    "\n",
    "# Run benchmarks\n",
    "latency, throughput, first_token_time = benchmark_model(model, input_text)\n",
    "\n",
    "# Print results\n",
    "print(\"LLaMA 7B Model - Avg Latency: {:.3f}s, Throughput: {:.2f} req/s, Time to First Token: {:.3f}s\".format(latency, throughput, first_token_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6dbacd-782c-4fdb-8cc0-7baf338c739e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed36d5d-a3ba-45e2-8fdb-d7d98a3180c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631b8cac-403a-47c3-bf34-80c73b4eb713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 09:22:15 pynccl.py:58] Loading nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 09:22:16,736\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint \"/data/data/vllm/vllm/model_executor/models/cus_llama_7b/checkpoints/consolidated.00.pth\"\n",
      "Loaded checkpoint in 8.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state dict in 4.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:08<00:00, 63.60it/s]\n",
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:06<00:00, 80.51it/s]\n",
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:06<00:00, 78.71it/s]\n",
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:06<00:00, 77.04it/s]\n",
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:07<00:00, 76.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA 7B Model - Avg Latency: 6.903s, Throughput: 0.14 req/s, Time to First Token: 7.009s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from vllm.model_executor.models.llama_for_vllm import LLaMAForVLLM\n",
    "\n",
    "# Define model and tokenizer paths\n",
    "model_dir = '/data/data/vllm/vllm/model_executor/models/cus_llama_7b/checkpoints/'\n",
    "tokenizer_dir = '/data/data/vllm/vllm/model_executor/models/cus_llama_7b/tokenizer/tokenizer.model'\n",
    "\n",
    "# Initialize the LLaMA tokenizer\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load(tokenizer_dir)\n",
    "\n",
    "# Initialize the LLaMA 7B model\n",
    "model = LLaMAForVLLM(model_dir, tokenizer_dir, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Input text for testing\n",
    "input_text = \"Translate English to French: Hi How are you, what's happening, tell me something new?\"\n",
    "\n",
    "# Function to benchmark a model\n",
    "def benchmark_model(model, text, iterations=3):\n",
    "    # Warm up model\n",
    "    for _ in range(2):\n",
    "        model.generate(text)\n",
    "\n",
    "    # Measure latency and time to first token\n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        start = time.time()\n",
    "        model.generate(text)\n",
    "        first_token_time = time.time() - start  # This simplifies first token time to full generation time\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    avg_latency = total_time / iterations\n",
    "    avg_first_token_time = first_token_time  # Simplistic assumption\n",
    "\n",
    "    # Throughput calculation\n",
    "    throughput = iterations / total_time\n",
    "\n",
    "    return avg_latency, throughput, avg_first_token_time\n",
    "\n",
    "# Run benchmarks\n",
    "latency, throughput, first_token_time = benchmark_model(model, input_text)\n",
    "\n",
    "# Print results\n",
    "print(\"LLaMA 7B Model - Avg Latency: {:.3f}s, Throughput: {:.2f} req/s, Time to First Token: {:.3f}s\".format(latency, throughput, first_token_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75131b0c-deb4-43cf-ad97-ea0e0e25b3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e4da2-18ed-4ad7-8c3e-be41951d3249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f291655-5997-43df-b130-bde1bc792b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d7479d-2b09-46fc-97f8-ace182069eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165e7dc-2a92-4eca-939d-1ac7be235feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b00edc-8caa-430a-b76f-bcc9129ab4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def print_tree(path, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively prints the directory tree structure.\n",
    "    \n",
    "    Args:\n",
    "        path (str or Path): The directory path to generate the tree structure.\n",
    "        indent (int): The indentation level for the current directory.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    \n",
    "    # Print the current directory\n",
    "    print(' ' * indent + '📂 ' + path.name)\n",
    "    \n",
    "    # Iterate through the contents of the directory\n",
    "    for item in path.iterdir():\n",
    "        if item.is_dir():\n",
    "            # Recursively print the subdirectory tree\n",
    "            print_tree(item, indent + 2)\n",
    "        else:\n",
    "            # Print the file\n",
    "            print(' ' * (indent + 2) + '📄 ' + item.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8a366-dbba-4b92-968f-8dd44e5cd50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree('/data/data/vllm/vllm/model_executor/models/llama_7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ec955-e06e-45c9-9faf-8484350aa6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2af2e-725c-4315-a6aa-430a00736f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5b81d-791c-46a1-9595-190c6a49841d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:07<00:00, 71.54it/s]\n",
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:07<00:00, 67.06it/s]\n",
      "STAGE:2024-05-15 09:23:45 33650:33650 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:16<00:00, 32.27it/s]\n",
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:16<00:00, 32.04it/s]\n",
      "STAGE:2024-05-15 09:24:33 33650:33650 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-15 09:24:36 33650:33650 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA 7B Model - Avg Latency: 253.306s, Throughput: 0.00 req/s, Time to First Token: 16.643s\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "def benchmark_model_with_profiler(model, text, iterations=2):\n",
    "    # Warm up model\n",
    "    for _ in range(2):\n",
    "        model.generate(text)\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        start_time = time.time()\n",
    "        for _ in range(iterations):\n",
    "            start = time.time()\n",
    "            model.generate(text)\n",
    "            first_token_time = time.time() - start  # This simplifies first token time to full generation time\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    avg_latency = total_time / iterations\n",
    "    avg_first_token_time = first_token_time  # Simplistic assumption\n",
    "\n",
    "    # Throughput calculation\n",
    "    throughput = iterations / total_time\n",
    "\n",
    "    print(\"LLaMA 7B Model - Avg Latency: {:.3f}s, Throughput: {:.2f} req/s, Time to First Token: {:.3f}s\".format(\n",
    "        avg_latency, throughput, avg_first_token_time))\n",
    "    \n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "# Run benchmarks with profiler\n",
    "latency, throughput, first_token_time = benchmark_model_with_profiler(model, input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bda15b-cd5f-4fac-a4f4-cbe83803d502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf74e09-1466-478c-8603-48fdc614a237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b79a0-656c-4d61-97db-534bed5b643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/data/vllm/vllm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002071cf-ff9d-4a3e-b100-4a67cc9c78d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaConfig\n",
    "# from vllm import SamplingParams\n",
    "# import torch\n",
    "\n",
    "# # Log in programmatically\n",
    "# login(token=\"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\")\n",
    "\n",
    "# # Initialize the model\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# config = LlamaConfig.from_pretrained(model_name, use_auth_token=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "# llama_model = LlamaForCausalLM(config=config)\n",
    "\n",
    "# # Define sampling parameters\n",
    "# sampling_params = SamplingParams(temperature=0.7, top_k=50)\n",
    "\n",
    "# # Prepare input prompts\n",
    "# prompts = [\"Translate the following English text to French: 'Hello, how are you?'\"]\n",
    "# inputs = tokenizer(prompts, return_tensors='pt', padding=True)\n",
    "\n",
    "# # Perform inference\n",
    "# with torch.no_grad():\n",
    "#     hidden_states = llama_model(\n",
    "#         input_ids=inputs['input_ids'],\n",
    "#         positions=torch.arange(inputs['input_ids'].size(1)).unsqueeze(0),\n",
    "#         kv_caches=[None] * config.num_hidden_layers,\n",
    "#         attn_metadata=None\n",
    "#     )\n",
    "#     logits = llama_model.compute_logits(hidden_states, None)\n",
    "\n",
    "# # Decode the output tokens\n",
    "# outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in logits.argmax(dim=-1)]\n",
    "\n",
    "# # Print the generated outputs\n",
    "# for output in outputs:\n",
    "#     print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13575047-8137-4429-8802-601a0539a498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78d6bc-7268-4a8e-953f-ba79968e8b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc24771-b214-489a-9e50-5212deedef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    " \n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# # model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_id,\n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#     device_map=\"auto\",\n",
    "#     token=\"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\"\n",
    "# )\n",
    " \n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "# ]\n",
    " \n",
    "# prompt = pipeline.tokenizer.apply_chat_template(\n",
    "#         messages, \n",
    "#         tokenize=False, \n",
    "#         add_generation_prompt=True\n",
    "# )\n",
    " \n",
    "# terminators = [\n",
    "#     pipeline.tokenizer.eos_token_id,\n",
    "#     pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "# ]\n",
    " \n",
    "# outputs = pipeline(\n",
    "#     prompt,\n",
    "#     max_new_tokens=256,\n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9,\n",
    "# )\n",
    "# print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59505632-d9f0-46a2-adc4-4e86bfad4a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9613cad0-bbd7-4b2c-9d7a-5f7661f6341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# from transformers import AutoTokenizer, LlamaConfig\n",
    "# from vllm import SamplingParams\n",
    "# from vllm.model_executor.models.llama import LlamaForCausalLM\n",
    "# import torch\n",
    "# import torch.distributed as dist\n",
    "\n",
    "# # Print working directory and Python path\n",
    "# print(\"Working Directory:\", os.getcwd())\n",
    "# print(\"Python Path:\", sys.path)\n",
    "\n",
    "# # Initialize the distributed environment\n",
    "# dist.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "# # Model name\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# # Load the model configuration and tokenizer\n",
    "# config = LlamaConfig.from_pretrained(model_name, use_auth_token=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "# # Initialize the model\n",
    "# llama_model = LlamaForCausalLM(config=config)\n",
    "\n",
    "# # Define sampling parameters\n",
    "# sampling_params = SamplingParams(temperature=0.7, top_k=50, max_length=100)\n",
    "\n",
    "# # Prepare input prompts\n",
    "# prompts = [\"Translate the following English text to French: 'Hello, how are you?'\"]\n",
    "\n",
    "# # Tokenize prompts\n",
    "# input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True).input_ids.to(\"cuda\")\n",
    "\n",
    "# # Perform inference\n",
    "# with torch.no_grad():\n",
    "#     outputs = llama_model.model(input_ids=input_ids, positions=torch.arange(input_ids.size(1)).unsqueeze(0).to(\"cuda\"), kv_caches=[None] * config.num_hidden_layers, attn_metadata=None)\n",
    "\n",
    "# # Decode and print the generated outputs\n",
    "# for output in outputs:\n",
    "#     print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "# # Clean up distributed environment\n",
    "# dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fdc9dc5-b2c3-4a30-ae5e-fec7987473fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 08:55:27 pynccl.py:58] Loading nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 08:55:28,927\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 08:55:29 llm_engine.py:84] Initializing an LLM engine (v0.4.0.post1) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n",
      "INFO 05-15 08:55:29 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 05-15 08:55:29 selector.py:33] Using XFormers backend.\n",
      "Loading model class for architecture: LlamaForCausalLM\n",
      "INFO 05-15 08:55:31 weight_utils.py:197] Using model weights format ['*.safetensors']\n",
      "INFO 05-15 08:55:34 model_runner.py:169] Loading model weights took 12.5523 GB\n",
      "INFO 05-15 08:55:35 gpu_executor.py:61] # GPU blocks: 7383, # CPU blocks: 512\n",
      "INFO 05-15 08:55:37 model_runner.py:967] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-15 08:55:37 model_runner.py:971] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-15 08:55:41 model_runner.py:1048] Graph capturing finished in 4 secs.\n",
      "Loading model class for architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestOutput(request_id=0, prompt=\"Translate the following English text to French: 'Hello, how are you?'\", prompt_token_ids=[1, 4103, 9632, 278, 1494, 4223, 1426, 304, 5176, 29901, 525, 10994, 29892, 920, 526, 366, 17901], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"\\n\\nTranslation: 'Bonjour, comment vas-tu?'\", token_ids=[13, 13, 4300, 18411, 29901, 525, 29933, 265, 29926, 473, 29892, 3440, 19723, 29899, 9161, 17901], cumulative_logprob=-1.4840798745026333, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1715763341.4208834, last_token_time=1715763341.4208834, first_scheduled_time=1715763341.473141, first_token_time=1715763341.5141234, time_in_queue=0.052257537841796875, finished_time=1715763341.6303942), lora_request=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize the model\n",
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(temperature=0.7, top_k=50)\n",
    "\n",
    "# Prepare input prompts\n",
    "prompts = [\"Translate the following English text to French: 'Hello, how are you?'\"]\n",
    "\n",
    "# Perform inference\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the generated outputs\n",
    "for output in outputs:\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7b485-a4b6-413a-bfa4-535c40370b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61042e82-ca47-4763-b1e2-626068c0650d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b2f111-073d-48d3-a867-9402f684c1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 09:20:13 llm_engine.py:84] Initializing an LLM engine (v0.4.0.post1) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n",
      "Loading model class for architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 82.50 MiB is free. Process 3118876 has 79.01 GiB memory in use. Of the allocated memory 77.12 GiB is allocated by PyTorch, and 31.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-7b-chat-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Define sampling parameters\u001b[39;00m\n\u001b[1;32m     13\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[0;32m/data/data/vllm/vllm/entrypoints/llm.py:112\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     94\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m     95\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     96\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    111\u001b[0m )\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/data/data/vllm/vllm/engine/llm_engine.py:238\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    235\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/data/data/vllm/vllm/engine/llm_engine.py:125\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, vision_language_config, speculative_config, decoding_config, tensorizer_config, executor_class, log_stats, usage_context)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenizer \u001b[38;5;241m=\u001b[39m Detokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_language_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorizer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorizer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n",
      "File \u001b[0;32m/data/data/vllm/vllm/executor/executor_base.py:41\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, vision_language_config, speculative_config, tensorizer_config)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeculative_config \u001b[38;5;241m=\u001b[39m speculative_config\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensorizer_config \u001b[38;5;241m=\u001b[39m tensorizer_config\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/data/vllm/vllm/executor/gpu_executor.py:20\u001b[0m, in \u001b[0;36mGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeculative_config\n\u001b[1;32m     17\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeculative decoding not yet supported for GPU backend\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Instantiate the worker and load the model to GPU.\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/data/vllm/vllm/executor/gpu_executor.py:47\u001b[0m, in \u001b[0;36mGPUExecutor._init_worker\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m Worker(\n\u001b[1;32m     33\u001b[0m     model_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config,\n\u001b[1;32m     34\u001b[0m     parallel_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     is_driver_worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39minit_device()\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/data/vllm/vllm/worker/worker.py:113\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/data/vllm/vllm/worker/model_runner.py:158\u001b[0m, in \u001b[0;36mModelRunner.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CudaMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_language_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtensorizer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensorizer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m    169\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/model_loader.py:105\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config, device_config, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m         model \u001b[38;5;241m=\u001b[39m load_with_tensorizer(tensorizer_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kwargs)\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 105\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlinear_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinear_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mload_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdummy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# NOTE(woosuk): For accurate performance evaluation, we assign\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# random values to the weights.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     initialize_dummy_weights(model)\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/models/llama.py:335\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config, linear_method, lora_config)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_method \u001b[38;5;241m=\u001b[39m linear_method\n\u001b[0;32m--> 335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpadded_vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lora_config:\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/models/llama.py:263\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config, linear_method, lora_config)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morg_vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    260\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    261\u001b[0m     org_num_embeddings\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    262\u001b[0m )\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m    264\u001b[0m     LlamaDecoderLayer(config, linear_method)\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    266\u001b[0m ])\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/models/llama.py:264\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morg_vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    260\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    261\u001b[0m     org_num_embeddings\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    262\u001b[0m )\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m--> 264\u001b[0m     \u001b[43mLlamaDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    266\u001b[0m ])\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/models/llama.py:203\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config, linear_method)\u001b[0m\n\u001b[1;32m    189\u001b[0m attention_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    190\u001b[0m     config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m LlamaAttention(\n\u001b[1;32m    192\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    193\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_attention_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     sliding_window\u001b[38;5;241m=\u001b[39msliding_window,\n\u001b[1;32m    202\u001b[0m )\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaMLP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinear_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinear_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    210\u001b[0m                                eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    212\u001b[0m                                         eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/models/llama.py:63\u001b[0m, in \u001b[0;36mLlamaMLP.__init__\u001b[0;34m(self, hidden_size, intermediate_size, hidden_act, linear_method)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     57\u001b[0m     hidden_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     linear_method: Optional[LinearMethodBase] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     61\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_up_proj \u001b[38;5;241m=\u001b[39m \u001b[43mMergedColumnParallelLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinear_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinear_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj \u001b[38;5;241m=\u001b[39m RowParallelLinear(intermediate_size,\n\u001b[1;32m     68\u001b[0m                                        hidden_size,\n\u001b[1;32m     69\u001b[0m                                        bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     70\u001b[0m                                        linear_method\u001b[38;5;241m=\u001b[39mlinear_method)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hidden_act \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/layers/linear.py:262\u001b[0m, in \u001b[0;36mMergedColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_sizes, bias, gather_output, skip_bias_add, params_dtype, linear_method)\u001b[0m\n\u001b[1;32m    260\u001b[0m tp_size \u001b[38;5;241m=\u001b[39m get_tensor_model_parallel_world_size()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(output_size \u001b[38;5;241m%\u001b[39m tp_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output_size \u001b[38;5;129;01min\u001b[39;00m output_sizes)\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgather_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mskip_bias_add\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/layers/linear.py:184\u001b[0m, in \u001b[0;36mColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_size, bias, gather_output, skip_bias_add, params_dtype, linear_method)\u001b[0m\n\u001b[1;32m    182\u001b[0m     linear_method \u001b[38;5;241m=\u001b[39m UnquantizedLinearMethod()\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_method \u001b[38;5;241m=\u001b[39m linear_method\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mweight_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[1;32m    193\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size_per_partition,\n\u001b[1;32m    194\u001b[0m                     dtype\u001b[38;5;241m=\u001b[39mparams_dtype))\n",
      "File \u001b[0;32m/data/data/vllm/vllm/model_executor/layers/linear.py:68\u001b[0m, in \u001b[0;36mUnquantizedLinearMethod.create_weights\u001b[0;34m(self, layer, input_size_per_partition, output_size_per_partition, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m     64\u001b[0m                    input_size_per_partition: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     65\u001b[0m                    output_size_per_partition: \u001b[38;5;28mint\u001b[39m, input_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     66\u001b[0m                    output_size: \u001b[38;5;28mint\u001b[39m, params_dtype: torch\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     67\u001b[0m                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_weight_attrs):\n\u001b[0;32m---> 68\u001b[0m     weight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     71\u001b[0m                        requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     72\u001b[0m     set_weight_attrs(weight, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m     73\u001b[0m     layer\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 82.50 MiB is free. Process 3118876 has 79.01 GiB memory in use. Of the allocated memory 77.12 GiB is allocated by PyTorch, and 31.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from transformers import AutoTokenizer, LlamaConfig\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "import torch.profiler\n",
    "\n",
    "# Initialize the model\n",
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(temperature=0.7, top_k=50)\n",
    "\n",
    "# Prepare input prompts\n",
    "prompts = [\"Translate the following English text to French: 'Hello, how are you?'\"]\n",
    "\n",
    "# Tokenize prompts with padding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True).input_ids.to(\"cuda\")\n",
    "\n",
    "# Define a function for the inference process\n",
    "def run_inference():\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for output in outputs:\n",
    "        print(output)\n",
    "\n",
    "# Use PyTorch profiler\n",
    "with torch.profiler.profile(\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler('./log_dir'),\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as profiler:\n",
    "    for _ in range(5):  # Run multiple iterations to collect profiling data\n",
    "        run_inference()\n",
    "        profiler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f20103-0c79-4d1a-ac32-ffc329244a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57218b28-0f02-484d-9716-9666ea6d2af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db52a216-7508-418e-a8db-34260b10a90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 09:20:39 pynccl.py:58] Loading nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 09:20:40,729\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 09:20:41 llm_engine.py:84] Initializing an LLM engine (v0.4.0.post1) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n",
      "INFO 05-15 09:20:41 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 05-15 09:20:41 selector.py:33] Using XFormers backend.\n",
      "Loading model class for architecture: LlamaForCausalLM\n",
      "INFO 05-15 09:20:43 weight_utils.py:197] Using model weights format ['*.safetensors']\n",
      "INFO 05-15 09:20:46 model_runner.py:169] Loading model weights took 12.5523 GB\n",
      "INFO 05-15 09:20:47 gpu_executor.py:61] # GPU blocks: 7383, # CPU blocks: 512\n",
      "INFO 05-15 09:20:49 model_runner.py:967] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-15 09:20:49 model_runner.py:971] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-15 09:20:53 model_runner.py:1048] Graph capturing finished in 4 secs.\n",
      "Loading model class for architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestOutput(request_id=0, prompt=\"Translate the following English text to French: 'Hello, how are you?'\", prompt_token_ids=[1, 4103, 9632, 278, 1494, 4223, 1426, 304, 5176, 29901, 525, 10994, 29892, 920, 526, 366, 17901], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"\\n\\nTranslation: 'Bonjour, comment vas-tu?'\", token_ids=[13, 13, 4300, 18411, 29901, 525, 29933, 265, 29926, 473, 29892, 3440, 19723, 29899, 9161, 17901], cumulative_logprob=-1.4840798745026333, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1715764853.227268, last_token_time=1715764853.227268, first_scheduled_time=1715764853.2323537, first_token_time=1715764853.268894, time_in_queue=0.00508570671081543, finished_time=1715764853.3845747), lora_request=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.75it/s]\n",
      "STAGE:2024-05-15 09:20:53 33218:33218 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestOutput(request_id=1, prompt=\"Translate the following English text to French: 'Hello, how are you?'\", prompt_token_ids=[1, 4103, 9632, 278, 1494, 4223, 1426, 304, 5176, 29901, 525, 10994, 29892, 920, 526, 366, 17901], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"\\n\\nFrench translation: 'Bonjour, comment allez-\", token_ids=[13, 13, 29943, 4615, 13962, 29901, 525, 29933, 265, 29926, 473, 29892, 3440, 4788, 29920, 29899], cumulative_logprob=-8.493001393127543, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1715764853.426331, last_token_time=1715764853.426331, first_scheduled_time=1715764853.431666, first_token_time=1715764853.4555779, time_in_queue=0.0053348541259765625, finished_time=1715764853.5779195), lora_request=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestOutput(request_id=2, prompt=\"Translate the following English text to French: 'Hello, how are you?'\", prompt_token_ids=[1, 4103, 9632, 278, 1494, 4223, 1426, 304, 5176, 29901, 525, 10994, 29892, 920, 526, 366, 17901], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"\\nTo translate the text 'Hello, how are you?' from English to French\", token_ids=[13, 1762, 14240, 278, 1426, 525, 10994, 29892, 920, 526, 366, 17901, 515, 4223, 304, 5176], cumulative_logprob=-8.969812831697823, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1715764853.5823615, last_token_time=1715764853.5823615, first_scheduled_time=1715764853.5904427, first_token_time=1715764853.6255414, time_in_queue=0.008081197738647461, finished_time=1715764853.7564552), lora_request=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestOutput(request_id=3, prompt=\"Translate the following English text to French: 'Hello, how are you?'\", prompt_token_ids=[1, 4103, 9632, 278, 1494, 4223, 1426, 304, 5176, 29901, 525, 10994, 29892, 920, 526, 366, 17901], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n\\nAnswer:\\nBonjour, comment vas-tu?', token_ids=[13, 13, 22550, 29901, 13, 29933, 265, 29926, 473, 29892, 3440, 19723, 29899, 9161, 29973, 2], cumulative_logprob=-3.2370567210091394, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1715764853.7606833, last_token_time=1715764853.7606833, first_scheduled_time=1715764853.765308, first_token_time=1715764853.7994626, time_in_queue=0.004624605178833008, finished_time=1715764853.932414), lora_request=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestOutput(request_id=4, prompt=\"Translate the following English text to French: 'Hello, how are you?'\", prompt_token_ids=[1, 4103, 9632, 278, 1494, 4223, 1426, 304, 5176, 29901, 525, 10994, 29892, 920, 526, 366, 17901], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"\\n\\nTranslation: 'Bonjour, comment allez-vous\", token_ids=[13, 13, 4300, 18411, 29901, 525, 29933, 265, 29926, 473, 29892, 3440, 4788, 29920, 29899, 23088], cumulative_logprob=-3.0459638472229926, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1715764853.9371033, last_token_time=1715764853.9371033, first_scheduled_time=1715764853.9424868, first_token_time=1715764853.9753585, time_in_queue=0.005383491516113281, finished_time=1715764854.105905), lora_request=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-15 09:20:54 33218:33218 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-15 09:20:54 33218:33218 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  name  cpu_time_total  \\\n",
      "0                                        ProfilerStep*          528146   \n",
      "1                                          aten::empty            4546   \n",
      "2                                             aten::to          224013   \n",
      "3                                       aten::_to_copy          221322   \n",
      "4                                  aten::empty_strided           11422   \n",
      "..                                                 ...             ...   \n",
      "133  void vllm::paged_attention_v2_kernel<unsigned ...               0   \n",
      "134  void vllm::paged_attention_v2_reduce_kernel<un...               0   \n",
      "135                                      memcpy32_post               0   \n",
      "136                               cudaDriverGetVersion               0   \n",
      "137                              cudaDeviceSynchronize              40   \n",
      "\n",
      "     cuda_time_total  self_cpu_time_total  self_cuda_time_total  \\\n",
      "0              33760               145751                     0   \n",
      "1                  0                 4052                     0   \n",
      "2               1619                29556                     0   \n",
      "3               1626                 6992                     0   \n",
      "4                  0                11422                     0   \n",
      "..               ...                  ...                   ...   \n",
      "133             8586                    0                  8586   \n",
      "134             1440                    0                  1440   \n",
      "135             1440                    0                  1440   \n",
      "136                0                    0                     0   \n",
      "137                0                   40                     0   \n",
      "\n",
      "     cpu_memory_usage  cuda_memory_usage  \n",
      "0                   0                  0  \n",
      "1                2240           36228096  \n",
      "2                1180           12856320  \n",
      "3                1468           12856320  \n",
      "4                1168           58444800  \n",
      "..                ...                ...  \n",
      "133                 0                  0  \n",
      "134                 0                  0  \n",
      "135                 0                  0  \n",
      "136                 0                  0  \n",
      "137                 0                  0  \n",
      "\n",
      "[138 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "import torch.profiler\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the model\n",
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(temperature=0.7, top_k=50)\n",
    "\n",
    "# Prepare input prompts\n",
    "prompts = [\"Translate the following English text to French: 'Hello, how are you?'\"]\n",
    "\n",
    "# Tokenize prompts with padding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True).input_ids.to(\"cuda\")\n",
    "\n",
    "# Define a function for the inference process\n",
    "def run_inference():\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for output in outputs:\n",
    "        print(output)\n",
    "\n",
    "# Use PyTorch profiler\n",
    "with torch.profiler.profile(\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler('./log_dir'),\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as profiler:\n",
    "    for _ in range(5):  # Run multiple iterations to collect profiling data\n",
    "        run_inference()\n",
    "        profiler.step()\n",
    "\n",
    "# Extract profiling data to a DataFrame\n",
    "events = profiler.key_averages()\n",
    "rows = []\n",
    "for event in events:\n",
    "    rows.append({\n",
    "        \"name\": event.key,\n",
    "        \"cpu_time_total\": event.cpu_time_total,\n",
    "        \"cuda_time_total\": event.cuda_time_total,\n",
    "        \"self_cpu_time_total\": event.self_cpu_time_total,\n",
    "        \"self_cuda_time_total\": event.self_cuda_time_total,\n",
    "        \"cpu_memory_usage\": event.cpu_memory_usage,\n",
    "        \"cuda_memory_usage\": event.cuda_memory_usage\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09b3554-c1a7-4066-8e7d-d074f9e3e1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cpu_time_total</th>\n",
       "      <th>cuda_time_total</th>\n",
       "      <th>self_cpu_time_total</th>\n",
       "      <th>self_cuda_time_total</th>\n",
       "      <th>cpu_memory_usage</th>\n",
       "      <th>cuda_memory_usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ProfilerStep*</td>\n",
       "      <td>528146</td>\n",
       "      <td>33760</td>\n",
       "      <td>145751</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aten::empty</td>\n",
       "      <td>4546</td>\n",
       "      <td>0</td>\n",
       "      <td>4052</td>\n",
       "      <td>0</td>\n",
       "      <td>2240</td>\n",
       "      <td>36228096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aten::to</td>\n",
       "      <td>224013</td>\n",
       "      <td>1619</td>\n",
       "      <td>29556</td>\n",
       "      <td>0</td>\n",
       "      <td>1180</td>\n",
       "      <td>12856320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aten::_to_copy</td>\n",
       "      <td>221322</td>\n",
       "      <td>1626</td>\n",
       "      <td>6992</td>\n",
       "      <td>0</td>\n",
       "      <td>1468</td>\n",
       "      <td>12856320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aten::empty_strided</td>\n",
       "      <td>11422</td>\n",
       "      <td>0</td>\n",
       "      <td>11422</td>\n",
       "      <td>0</td>\n",
       "      <td>1168</td>\n",
       "      <td>58444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>void vllm::paged_attention_v2_kernel&lt;unsigned ...</td>\n",
       "      <td>0</td>\n",
       "      <td>8586</td>\n",
       "      <td>0</td>\n",
       "      <td>8586</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>void vllm::paged_attention_v2_reduce_kernel&lt;un...</td>\n",
       "      <td>0</td>\n",
       "      <td>1440</td>\n",
       "      <td>0</td>\n",
       "      <td>1440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>memcpy32_post</td>\n",
       "      <td>0</td>\n",
       "      <td>1440</td>\n",
       "      <td>0</td>\n",
       "      <td>1440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>cudaDriverGetVersion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>cudaDeviceSynchronize</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  name  cpu_time_total  \\\n",
       "0                                        ProfilerStep*          528146   \n",
       "1                                          aten::empty            4546   \n",
       "2                                             aten::to          224013   \n",
       "3                                       aten::_to_copy          221322   \n",
       "4                                  aten::empty_strided           11422   \n",
       "..                                                 ...             ...   \n",
       "133  void vllm::paged_attention_v2_kernel<unsigned ...               0   \n",
       "134  void vllm::paged_attention_v2_reduce_kernel<un...               0   \n",
       "135                                      memcpy32_post               0   \n",
       "136                               cudaDriverGetVersion               0   \n",
       "137                              cudaDeviceSynchronize              40   \n",
       "\n",
       "     cuda_time_total  self_cpu_time_total  self_cuda_time_total  \\\n",
       "0              33760               145751                     0   \n",
       "1                  0                 4052                     0   \n",
       "2               1619                29556                     0   \n",
       "3               1626                 6992                     0   \n",
       "4                  0                11422                     0   \n",
       "..               ...                  ...                   ...   \n",
       "133             8586                    0                  8586   \n",
       "134             1440                    0                  1440   \n",
       "135             1440                    0                  1440   \n",
       "136                0                    0                     0   \n",
       "137                0                   40                     0   \n",
       "\n",
       "     cpu_memory_usage  cuda_memory_usage  \n",
       "0                   0                  0  \n",
       "1                2240           36228096  \n",
       "2                1180           12856320  \n",
       "3                1468           12856320  \n",
       "4                1168           58444800  \n",
       "..                ...                ...  \n",
       "133                 0                  0  \n",
       "134                 0                  0  \n",
       "135                 0                  0  \n",
       "136                 0                  0  \n",
       "137                 0                  0  \n",
       "\n",
       "[138 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7b573-60d1-44dc-a6de-fc62d558965d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6111ba90-71ef-4e43-a7bb-2b2fdc8af832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2c69d9-1d7c-493b-ba52-97b5815a39f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 09:40:55 pynccl.py:58] Loading nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 09:40:56,558\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint \"/data/data/vllm/vllm/model_executor/models/cus_llama_7b/checkpoints/consolidated.00.pth\"\n",
      "Loaded checkpoint in 8.38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state dict in 4.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:08<00:00, 60.54it/s]\n",
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:07<00:00, 70.56it/s]\n",
      "STAGE:2024-05-15 09:41:26 34517:34517 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:15<00:00, 33.59it/s]\n",
      "Generating tokens: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 533/533 [00:16<00:00, 33.11it/s]\n",
      "STAGE:2024-05-15 09:42:12 34517:34517 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-15 09:42:16 34517:34517 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA 7B Model - Avg Latency: 15.990s, Throughput: 0.06 req/s\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::matmul         5.51%        1.371s        33.14%        8.252s      26.786us       0.000us         0.00%        6.753s      21.920us        308074  \n",
      "                                           aten::linear         3.61%     898.583ms        28.82%        7.176s      29.917us       0.000us         0.00%        5.918s      24.675us        239850  \n",
      "                                               aten::mm        14.81%        3.686s        20.15%        5.016s      20.914us        6.076s        72.37%        6.435s      26.828us        239850  \n",
      "                                               aten::to         3.54%     881.756ms        18.81%        4.684s       9.678us       0.000us         0.00%     758.637ms       1.568us        483972  \n",
      "                                         aten::_to_copy         4.62%        1.149s        17.70%        4.408s      12.761us       0.000us         0.00%     851.792ms       2.466us        345392  \n",
      "                                       cudaLaunchKernel        15.17%        3.778s        15.17%        3.778s       3.326us     511.504ms         6.09%     511.569ms       0.450us       1136018  \n",
      "                                            aten::copy_         5.59%        1.391s        11.99%        2.986s       7.181us     840.851ms        10.02%        1.028s       2.474us        415750  \n",
      "                                          aten::type_as         0.78%     194.108ms         9.73%        2.422s      14.111us       0.000us         0.00%     351.446ms       2.048us        171626  \n",
      "                                              aten::mul         5.84%        1.453s         9.00%        2.240s       9.299us     393.972ms         4.69%     501.276ms       2.081us        240916  \n",
      "                                              aten::add         3.41%     848.074ms         5.24%        1.304s       9.483us     157.383ms         1.87%     219.241ms       1.594us        137514  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 24.899s\n",
      "Self CUDA time total: 8.395s\n",
      "\n",
      "                                                  name  cpu_time_total  \\\n",
      "0                                          aten::empty           17049   \n",
      "1                                             aten::to         4684082   \n",
      "2                                       aten::_to_copy         4407510   \n",
      "3                                  aten::empty_strided         1131969   \n",
      "4                                          aten::copy_         2985532   \n",
      "..                                                 ...             ...   \n",
      "154  void cutlass::Kernel<cutlass_80_wmma_tensorop_...               0   \n",
      "155  void (anonymous namespace)::softmax_warp_forwa...               0   \n",
      "156                                 aten::resolve_conj               0   \n",
      "157                                  aten::resolve_neg               0   \n",
      "158                              cudaDeviceSynchronize              12   \n",
      "\n",
      "     cuda_time_total  self_cpu_time_total  self_cuda_time_total  \\\n",
      "0                  0                17049                     0   \n",
      "1             758637               881756                     0   \n",
      "2             851792              1149427                     0   \n",
      "3                  0              1131969                     0   \n",
      "4            1028418              1391129                840851   \n",
      "..               ...                  ...                   ...   \n",
      "154             4791                    0                  4791   \n",
      "155             4224                    0                  4224   \n",
      "156                0                    0                     0   \n",
      "157                0                    0                     0   \n",
      "158                0                   12                     0   \n",
      "\n",
      "     cpu_memory_usage  cuda_memory_usage  \n",
      "0                   0                  0  \n",
      "1                   0                  0  \n",
      "2                   0                  0  \n",
      "3                   0                  0  \n",
      "4                   0                  0  \n",
      "..                ...                ...  \n",
      "154                 0                  0  \n",
      "155                 0                  0  \n",
      "156                 0                  0  \n",
      "157                 0                  0  \n",
      "158                 0                  0  \n",
      "\n",
      "[159 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from vllm.model_executor.models.llama_for_vllm import LLaMAForVLLM\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "# Define model and tokenizer paths\n",
    "model_dir = '/data/data/vllm/vllm/model_executor/models/cus_llama_7b/checkpoints/'\n",
    "tokenizer_dir = '/data/data/vllm/vllm/model_executor/models/cus_llama_7b/tokenizer/tokenizer.model'\n",
    "\n",
    "# Initialize the LLaMA tokenizer\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load(tokenizer_dir)\n",
    "\n",
    "# Initialize the LLaMA 7B model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = LLaMAForVLLM(model_dir, tokenizer_dir, device=device)\n",
    "\n",
    "# Input text for testing\n",
    "input_text = \"Translate English to French: Hi How are you, what's happening, tell me something new?\"\n",
    "\n",
    "# Function to benchmark a model\n",
    "def benchmark_model(model, text, iterations=5):\n",
    "    # Warm up model\n",
    "    for _ in range(2):\n",
    "        model.generate(text)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        model.generate(text)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    avg_latency = total_time / iterations\n",
    "    throughput = iterations / total_time\n",
    "\n",
    "    return avg_latency, throughput\n",
    "\n",
    "# Function to benchmark with profiler\n",
    "def benchmark_model_with_profiler(model, text, iterations=2):\n",
    "    # Warm up model\n",
    "    for _ in range(2):\n",
    "        model.generate(text)\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        start_time = time.time()\n",
    "        for _ in range(iterations):\n",
    "            model.generate(text)\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "    avg_latency = total_time / iterations\n",
    "    throughput = iterations / total_time\n",
    "\n",
    "    print(\"LLaMA 7B Model - Avg Latency: {:.3f}s, Throughput: {:.2f} req/s\".format(avg_latency, throughput))\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "    # Extract profiling data to a DataFrame\n",
    "    events = prof.key_averages()\n",
    "    rows = [{\n",
    "        \"name\": event.key,\n",
    "        \"cpu_time_total\": event.cpu_time_total,\n",
    "        \"cuda_time_total\": event.cuda_time_total,\n",
    "        \"self_cpu_time_total\": event.self_cpu_time_total,\n",
    "        \"self_cuda_time_total\": event.self_cuda_time_total,\n",
    "        \"cpu_memory_usage\": event.cpu_memory_usage,\n",
    "        \"cuda_memory_usage\": event.cuda_memory_usage\n",
    "    } for event in events]\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(df)\n",
    "\n",
    "# Run benchmarks\n",
    "# latency, throughput = benchmark_model(model, input_text)\n",
    "# print(\"LLaMA 7B Model - Avg Latency: {:.3f}s, Throughput: {:.2f} req/s\".format(latency, throughput))\n",
    "\n",
    "# Run benchmarks with profiler\n",
    "df = benchmark_model_with_profiler(model, input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e49621e6-18e1-44ec-b91d-dbe007cb2e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea32495-1b12-4d4d-ab32-9a7be8568620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
